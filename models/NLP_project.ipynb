{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nitRY9riXhIk",
        "outputId": "a5aac416-882f-448f-9c15-26024b7eebe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-01 15:28:57,618 : INFO : collecting all words and their counts\n",
            "2024-01-01 15:28:57,620 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2024-01-01 15:28:57,634 : INFO : PROGRESS: at sentence #10000, processed 39789 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,645 : INFO : PROGRESS: at sentence #20000, processed 79504 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,654 : INFO : PROGRESS: at sentence #30000, processed 119409 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,671 : INFO : PROGRESS: at sentence #40000, processed 159213 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,684 : INFO : PROGRESS: at sentence #50000, processed 199177 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,693 : INFO : PROGRESS: at sentence #60000, processed 238890 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,703 : INFO : PROGRESS: at sentence #70000, processed 278484 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,714 : INFO : PROGRESS: at sentence #80000, processed 318019 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,724 : INFO : PROGRESS: at sentence #90000, processed 358282 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,733 : INFO : PROGRESS: at sentence #100000, processed 397837 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,745 : INFO : PROGRESS: at sentence #110000, processed 437275 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,755 : INFO : PROGRESS: at sentence #120000, processed 477034 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,766 : INFO : PROGRESS: at sentence #130000, processed 516549 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,778 : INFO : PROGRESS: at sentence #140000, processed 556154 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,793 : INFO : PROGRESS: at sentence #150000, processed 595763 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,803 : INFO : PROGRESS: at sentence #160000, processed 635590 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,814 : INFO : PROGRESS: at sentence #170000, processed 675078 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,823 : INFO : PROGRESS: at sentence #180000, processed 714627 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,835 : INFO : PROGRESS: at sentence #190000, processed 754301 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,846 : INFO : PROGRESS: at sentence #200000, processed 794082 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,855 : INFO : PROGRESS: at sentence #210000, processed 833832 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,867 : INFO : PROGRESS: at sentence #220000, processed 873657 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,878 : INFO : PROGRESS: at sentence #230000, processed 913559 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,886 : INFO : PROGRESS: at sentence #240000, processed 952887 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,899 : INFO : PROGRESS: at sentence #250000, processed 992887 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,910 : INFO : PROGRESS: at sentence #260000, processed 1033054 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,920 : INFO : PROGRESS: at sentence #270000, processed 1072345 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,932 : INFO : PROGRESS: at sentence #280000, processed 1111668 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,942 : INFO : PROGRESS: at sentence #290000, processed 1151232 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,952 : INFO : PROGRESS: at sentence #300000, processed 1190646 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,963 : INFO : PROGRESS: at sentence #310000, processed 1230502 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,971 : INFO : PROGRESS: at sentence #320000, processed 1269998 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,986 : INFO : PROGRESS: at sentence #330000, processed 1309855 words, keeping 37 word types\n",
            "2024-01-01 15:28:57,997 : INFO : PROGRESS: at sentence #340000, processed 1349800 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,010 : INFO : PROGRESS: at sentence #350000, processed 1389345 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,020 : INFO : PROGRESS: at sentence #360000, processed 1428928 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,032 : INFO : PROGRESS: at sentence #370000, processed 1468377 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,047 : INFO : PROGRESS: at sentence #380000, processed 1508164 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,068 : INFO : PROGRESS: at sentence #390000, processed 1548218 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,097 : INFO : PROGRESS: at sentence #400000, processed 1587817 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,125 : INFO : PROGRESS: at sentence #410000, processed 1627522 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,144 : INFO : PROGRESS: at sentence #420000, processed 1666992 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,167 : INFO : PROGRESS: at sentence #430000, processed 1706547 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,185 : INFO : PROGRESS: at sentence #440000, processed 1746374 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,202 : INFO : PROGRESS: at sentence #450000, processed 1786268 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,220 : INFO : PROGRESS: at sentence #460000, processed 1826021 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,238 : INFO : PROGRESS: at sentence #470000, processed 1865512 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,254 : INFO : PROGRESS: at sentence #480000, processed 1905221 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,271 : INFO : PROGRESS: at sentence #490000, processed 1944821 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,290 : INFO : PROGRESS: at sentence #500000, processed 1984798 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,313 : INFO : PROGRESS: at sentence #510000, processed 2024679 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,330 : INFO : PROGRESS: at sentence #520000, processed 2064741 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,348 : INFO : PROGRESS: at sentence #530000, processed 2104349 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,364 : INFO : PROGRESS: at sentence #540000, processed 2143791 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,382 : INFO : PROGRESS: at sentence #550000, processed 2183137 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,394 : INFO : PROGRESS: at sentence #560000, processed 2222910 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,404 : INFO : PROGRESS: at sentence #570000, processed 2262661 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,414 : INFO : PROGRESS: at sentence #580000, processed 2301990 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,427 : INFO : PROGRESS: at sentence #590000, processed 2341673 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,438 : INFO : PROGRESS: at sentence #600000, processed 2381194 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,451 : INFO : PROGRESS: at sentence #610000, processed 2420633 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,462 : INFO : PROGRESS: at sentence #620000, processed 2460029 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,473 : INFO : PROGRESS: at sentence #630000, processed 2499926 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,484 : INFO : PROGRESS: at sentence #640000, processed 2539803 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,498 : INFO : PROGRESS: at sentence #650000, processed 2579565 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,508 : INFO : PROGRESS: at sentence #660000, processed 2619300 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,529 : INFO : PROGRESS: at sentence #670000, processed 2659180 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,542 : INFO : PROGRESS: at sentence #680000, processed 2698895 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,560 : INFO : PROGRESS: at sentence #690000, processed 2738873 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,568 : INFO : PROGRESS: at sentence #700000, processed 2778101 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,578 : INFO : PROGRESS: at sentence #710000, processed 2817552 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,588 : INFO : PROGRESS: at sentence #720000, processed 2856924 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,605 : INFO : PROGRESS: at sentence #730000, processed 2896457 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,616 : INFO : PROGRESS: at sentence #740000, processed 2936062 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,628 : INFO : PROGRESS: at sentence #750000, processed 2975628 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,639 : INFO : PROGRESS: at sentence #760000, processed 3015326 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,652 : INFO : PROGRESS: at sentence #770000, processed 3054984 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,664 : INFO : PROGRESS: at sentence #780000, processed 3094678 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,676 : INFO : PROGRESS: at sentence #790000, processed 3134247 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,685 : INFO : PROGRESS: at sentence #800000, processed 3173415 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,698 : INFO : PROGRESS: at sentence #810000, processed 3213036 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,708 : INFO : PROGRESS: at sentence #820000, processed 3252596 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,718 : INFO : PROGRESS: at sentence #830000, processed 3292479 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,728 : INFO : PROGRESS: at sentence #840000, processed 3332144 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,738 : INFO : PROGRESS: at sentence #850000, processed 3371890 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,751 : INFO : PROGRESS: at sentence #860000, processed 3411405 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,761 : INFO : PROGRESS: at sentence #870000, processed 3450922 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,771 : INFO : PROGRESS: at sentence #880000, processed 3490273 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,800 : INFO : PROGRESS: at sentence #890000, processed 3529901 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,817 : INFO : PROGRESS: at sentence #900000, processed 3569633 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,833 : INFO : PROGRESS: at sentence #910000, processed 3609405 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,846 : INFO : PROGRESS: at sentence #920000, processed 3649070 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,855 : INFO : PROGRESS: at sentence #930000, processed 3688835 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,875 : INFO : PROGRESS: at sentence #940000, processed 3729028 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,888 : INFO : PROGRESS: at sentence #950000, processed 3768766 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,899 : INFO : PROGRESS: at sentence #960000, processed 3808515 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,908 : INFO : PROGRESS: at sentence #970000, processed 3848122 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,918 : INFO : PROGRESS: at sentence #980000, processed 3888156 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,928 : INFO : PROGRESS: at sentence #990000, processed 3927889 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,938 : INFO : PROGRESS: at sentence #1000000, processed 3967805 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,948 : INFO : PROGRESS: at sentence #1010000, processed 4007568 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,960 : INFO : PROGRESS: at sentence #1020000, processed 4047032 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,969 : INFO : PROGRESS: at sentence #1030000, processed 4086707 words, keeping 37 word types\n",
            "2024-01-01 15:28:58,989 : INFO : PROGRESS: at sentence #1040000, processed 4126677 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,002 : INFO : PROGRESS: at sentence #1050000, processed 4166867 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,012 : INFO : PROGRESS: at sentence #1060000, processed 4206076 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,022 : INFO : PROGRESS: at sentence #1070000, processed 4245586 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,032 : INFO : PROGRESS: at sentence #1080000, processed 4285134 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,042 : INFO : PROGRESS: at sentence #1090000, processed 4325104 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,054 : INFO : PROGRESS: at sentence #1100000, processed 4364839 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,065 : INFO : PROGRESS: at sentence #1110000, processed 4404376 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,074 : INFO : PROGRESS: at sentence #1120000, processed 4444283 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,085 : INFO : PROGRESS: at sentence #1130000, processed 4484058 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,098 : INFO : PROGRESS: at sentence #1140000, processed 4523533 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,107 : INFO : PROGRESS: at sentence #1150000, processed 4563133 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,117 : INFO : PROGRESS: at sentence #1160000, processed 4603000 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,128 : INFO : PROGRESS: at sentence #1170000, processed 4642804 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,136 : INFO : PROGRESS: at sentence #1180000, processed 4682511 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,148 : INFO : PROGRESS: at sentence #1190000, processed 4722216 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,158 : INFO : PROGRESS: at sentence #1200000, processed 4762313 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,167 : INFO : PROGRESS: at sentence #1210000, processed 4801723 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,179 : INFO : PROGRESS: at sentence #1220000, processed 4841704 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,189 : INFO : PROGRESS: at sentence #1230000, processed 4881737 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,200 : INFO : PROGRESS: at sentence #1240000, processed 4921272 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,210 : INFO : PROGRESS: at sentence #1250000, processed 4961237 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,219 : INFO : PROGRESS: at sentence #1260000, processed 5000924 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,233 : INFO : PROGRESS: at sentence #1270000, processed 5040594 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,252 : INFO : PROGRESS: at sentence #1280000, processed 5080123 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,265 : INFO : PROGRESS: at sentence #1290000, processed 5120057 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,277 : INFO : PROGRESS: at sentence #1300000, processed 5160129 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,286 : INFO : PROGRESS: at sentence #1310000, processed 5199882 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,298 : INFO : PROGRESS: at sentence #1320000, processed 5239771 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,314 : INFO : PROGRESS: at sentence #1330000, processed 5279659 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,328 : INFO : PROGRESS: at sentence #1340000, processed 5319292 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,354 : INFO : PROGRESS: at sentence #1350000, processed 5359138 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,378 : INFO : PROGRESS: at sentence #1360000, processed 5398630 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,399 : INFO : PROGRESS: at sentence #1370000, processed 5438354 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,416 : INFO : PROGRESS: at sentence #1380000, processed 5478486 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,432 : INFO : PROGRESS: at sentence #1390000, processed 5517952 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,452 : INFO : PROGRESS: at sentence #1400000, processed 5557567 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,470 : INFO : PROGRESS: at sentence #1410000, processed 5597301 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,490 : INFO : PROGRESS: at sentence #1420000, processed 5636809 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,508 : INFO : PROGRESS: at sentence #1430000, processed 5676528 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,530 : INFO : PROGRESS: at sentence #1440000, processed 5716042 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,556 : INFO : PROGRESS: at sentence #1450000, processed 5755917 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,578 : INFO : PROGRESS: at sentence #1460000, processed 5795331 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,603 : INFO : PROGRESS: at sentence #1470000, processed 5835086 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,621 : INFO : PROGRESS: at sentence #1480000, processed 5874838 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,642 : INFO : PROGRESS: at sentence #1490000, processed 5914415 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,656 : INFO : PROGRESS: at sentence #1500000, processed 5954294 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,670 : INFO : PROGRESS: at sentence #1510000, processed 5994116 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,687 : INFO : PROGRESS: at sentence #1520000, processed 6033660 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,701 : INFO : PROGRESS: at sentence #1530000, processed 6073293 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,715 : INFO : PROGRESS: at sentence #1540000, processed 6113160 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,734 : INFO : PROGRESS: at sentence #1550000, processed 6152725 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,751 : INFO : PROGRESS: at sentence #1560000, processed 6192512 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,767 : INFO : PROGRESS: at sentence #1570000, processed 6231998 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,784 : INFO : PROGRESS: at sentence #1580000, processed 6271740 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,802 : INFO : PROGRESS: at sentence #1590000, processed 6311521 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,812 : INFO : PROGRESS: at sentence #1600000, processed 6351673 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,826 : INFO : PROGRESS: at sentence #1610000, processed 6391580 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,841 : INFO : PROGRESS: at sentence #1620000, processed 6431140 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,860 : INFO : PROGRESS: at sentence #1630000, processed 6470506 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,880 : INFO : PROGRESS: at sentence #1640000, processed 6510145 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,894 : INFO : PROGRESS: at sentence #1650000, processed 6550219 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,905 : INFO : PROGRESS: at sentence #1660000, processed 6589881 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,916 : INFO : PROGRESS: at sentence #1670000, processed 6629610 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,927 : INFO : PROGRESS: at sentence #1680000, processed 6669268 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,939 : INFO : PROGRESS: at sentence #1690000, processed 6709023 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,951 : INFO : PROGRESS: at sentence #1700000, processed 6748647 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,968 : INFO : PROGRESS: at sentence #1710000, processed 6788198 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,983 : INFO : PROGRESS: at sentence #1720000, processed 6828244 words, keeping 37 word types\n",
            "2024-01-01 15:28:59,995 : INFO : PROGRESS: at sentence #1730000, processed 6868399 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,011 : INFO : PROGRESS: at sentence #1740000, processed 6907915 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,027 : INFO : PROGRESS: at sentence #1750000, processed 6947456 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,041 : INFO : PROGRESS: at sentence #1760000, processed 6986928 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,059 : INFO : PROGRESS: at sentence #1770000, processed 7026801 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,072 : INFO : PROGRESS: at sentence #1780000, processed 7066400 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,094 : INFO : PROGRESS: at sentence #1790000, processed 7106028 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,105 : INFO : PROGRESS: at sentence #1800000, processed 7145639 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,121 : INFO : PROGRESS: at sentence #1810000, processed 7185106 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,138 : INFO : PROGRESS: at sentence #1820000, processed 7224781 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,169 : INFO : PROGRESS: at sentence #1830000, processed 7264511 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,196 : INFO : PROGRESS: at sentence #1840000, processed 7304089 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,207 : INFO : PROGRESS: at sentence #1850000, processed 7343290 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,225 : INFO : PROGRESS: at sentence #1860000, processed 7383079 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,239 : INFO : PROGRESS: at sentence #1870000, processed 7423161 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,249 : INFO : PROGRESS: at sentence #1880000, processed 7462753 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,267 : INFO : PROGRESS: at sentence #1890000, processed 7502483 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,281 : INFO : PROGRESS: at sentence #1900000, processed 7542173 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,292 : INFO : PROGRESS: at sentence #1910000, processed 7581763 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,305 : INFO : PROGRESS: at sentence #1920000, processed 7621382 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,316 : INFO : PROGRESS: at sentence #1930000, processed 7661456 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,326 : INFO : PROGRESS: at sentence #1940000, processed 7701323 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,342 : INFO : PROGRESS: at sentence #1950000, processed 7740868 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,352 : INFO : PROGRESS: at sentence #1960000, processed 7780679 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,363 : INFO : PROGRESS: at sentence #1970000, processed 7820265 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,371 : INFO : PROGRESS: at sentence #1980000, processed 7859691 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,384 : INFO : PROGRESS: at sentence #1990000, processed 7899377 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,395 : INFO : PROGRESS: at sentence #2000000, processed 7938930 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,404 : INFO : PROGRESS: at sentence #2010000, processed 7978691 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,415 : INFO : PROGRESS: at sentence #2020000, processed 8018714 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,426 : INFO : PROGRESS: at sentence #2030000, processed 8058366 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,436 : INFO : PROGRESS: at sentence #2040000, processed 8098031 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,446 : INFO : PROGRESS: at sentence #2050000, processed 8137699 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,456 : INFO : PROGRESS: at sentence #2060000, processed 8177398 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,467 : INFO : PROGRESS: at sentence #2070000, processed 8217354 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,477 : INFO : PROGRESS: at sentence #2080000, processed 8257308 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,488 : INFO : PROGRESS: at sentence #2090000, processed 8296798 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,499 : INFO : PROGRESS: at sentence #2100000, processed 8336716 words, keeping 37 word types\n",
            "2024-01-01 15:29:00,503 : INFO : collected 37 word types from a corpus of 8353805 raw words and 2104234 sentences\n",
            "2024-01-01 15:29:00,504 : INFO : Creating a fresh vocabulary\n",
            "2024-01-01 15:29:00,504 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 37 unique words (100.00% of original 37, drops 0)', 'datetime': '2024-01-01T15:29:00.504846', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 15:29:00,506 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 8353805 word corpus (100.00% of original 8353805, drops 0)', 'datetime': '2024-01-01T15:29:00.506393', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 15:29:00,509 : INFO : deleting the raw counts dictionary of 37 items\n",
            "2024-01-01 15:29:00,509 : INFO : sample=0.001 downsamples 32 most-common words\n",
            "2024-01-01 15:29:00,510 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 1669253.6281965326 word corpus (20.0%% of prior 8353805)', 'datetime': '2024-01-01T15:29:00.510843', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 15:29:00,515 : INFO : estimated required memory for 37 words, 2000000 buckets and 10 dimensions: 80025372 bytes\n",
            "2024-01-01 15:29:00,516 : INFO : resetting layer weights\n",
            "2024-01-01 15:29:00,633 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-01T15:29:00.633017', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
            "2024-01-01 15:29:00,634 : INFO : FastText lifecycle event {'msg': 'training model with 5 workers on 37 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-01-01T15:29:00.634025', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
            "2024-01-01 15:29:01,673 : INFO : EPOCH 0 - PROGRESS: at 35.09% examples, 584521 words/s, in_qsize 10, out_qsize 0\n",
            "2024-01-01 15:29:02,676 : INFO : EPOCH 0 - PROGRESS: at 71.46% examples, 595167 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 15:29:03,416 : INFO : EPOCH 0: training on 8353805 raw words (1668303 effective words) took 2.7s, 608088 effective words/s\n",
            "2024-01-01 15:29:04,466 : INFO : EPOCH 1 - PROGRESS: at 37.73% examples, 614427 words/s, in_qsize 10, out_qsize 0\n",
            "2024-01-01 15:29:05,475 : INFO : EPOCH 1 - PROGRESS: at 75.89% examples, 622877 words/s, in_qsize 0, out_qsize 8\n",
            "2024-01-01 15:29:06,067 : INFO : EPOCH 1: training on 8353805 raw words (1668444 effective words) took 2.6s, 635758 effective words/s\n",
            "2024-01-01 15:29:07,101 : INFO : EPOCH 2 - PROGRESS: at 37.85% examples, 629376 words/s, in_qsize 10, out_qsize 1\n",
            "2024-01-01 15:29:08,102 : INFO : EPOCH 2 - PROGRESS: at 74.69% examples, 622543 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 15:29:08,741 : INFO : EPOCH 2: training on 8353805 raw words (1669014 effective words) took 2.6s, 631447 effective words/s\n",
            "2024-01-01 15:29:09,753 : INFO : EPOCH 3 - PROGRESS: at 37.49% examples, 623664 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 15:29:10,754 : INFO : EPOCH 3 - PROGRESS: at 74.57% examples, 621129 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 15:29:11,396 : INFO : EPOCH 3: training on 8353805 raw words (1668302 effective words) took 2.6s, 630842 effective words/s\n",
            "2024-01-01 15:29:12,427 : INFO : EPOCH 4 - PROGRESS: at 37.73% examples, 627847 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 15:29:13,437 : INFO : EPOCH 4 - PROGRESS: at 74.93% examples, 621293 words/s, in_qsize 9, out_qsize 4\n",
            "2024-01-01 15:29:14,072 : INFO : EPOCH 4: training on 8353805 raw words (1668559 effective words) took 2.6s, 630235 effective words/s\n",
            "2024-01-01 15:29:14,073 : INFO : FastText lifecycle event {'msg': 'training on 41769025 raw words (8342622 effective words) took 13.4s, 620823 effective words/s', 'datetime': '2024-01-01T15:29:14.073067', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
            "2024-01-01 15:29:14,073 : INFO : FastText lifecycle event {'params': 'FastText<vocab=37, vector_size=10, alpha=0.025>', 'datetime': '2024-01-01T15:29:14.073067', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8353805\n",
            "و\n",
            "6\n",
            "[-0.05952365  0.7864038  -0.30884534  0.00966583 -0.31647366 -0.37679344\n",
            "  0.14028403 -0.0656407   0.3308646   0.13131635]\n"
          ]
        }
      ],
      "source": [
        "# %pip uninstall tensorflow\n",
        "# %pip install tensorflow\n",
        "# %pip install keras\n",
        "# %pip install gensim\n",
        "# %pip install nltk\n",
        "# %pip install torch\n",
        "# %pip install fasttext\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "from datetime import datetime\n",
        "from gensim.models import *\n",
        "import logging\n",
        "import fasttext\n",
        "# from rnn_utils import *\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "''' D_NAMES: This is a list containing names of various Arabic diacritics. Each\n",
        " element of the list represents a specific diacritic type. '''\n",
        "D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping\n",
        "from diacritic names to their corresponding Unicode characters.'''\n",
        "NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''\n",
        "DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode\n",
        " characters of all the diacritics.'''\n",
        "ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())\n",
        "\n",
        "\n",
        "# Remove all standard diacritics from the text, leaving the letters only.\n",
        "def clear_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    return ''.join([l for l in text if l not in ARABIC_DIACRITICS])\n",
        "\n",
        "\n",
        "# Return the diacritics from the text while keeping their original positions.\n",
        "def extract_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    diacritics = []\n",
        "    classes = []\n",
        "    temp = ''\n",
        "    for i in range(1, len(text)):\n",
        "        temp = ''\n",
        "        if text[i] in ARABIC_DIACRITICS:\n",
        "            if text[i-1] == NAME2DIACRITIC['Shadda']:\n",
        "                diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                if (temp == ('Shadda', 'Fatha')):\n",
        "                    classes.pop()\n",
        "                    classes.append(8)\n",
        "                elif (temp == ('Shadda', 'Fathatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(9)\n",
        "                elif (temp == ('Shadda', 'Damma')):\n",
        "                    classes.pop()\n",
        "                    classes.append(10)\n",
        "                elif (temp == ('Shadda', 'Dammatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(11)\n",
        "                elif (temp == ('Shadda', 'Kasra')):\n",
        "                    classes.pop()\n",
        "                    classes.append(12)\n",
        "                elif (temp == ('Shadda', 'Kasratan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(13)\n",
        "            else:\n",
        "                diacritics.append(DIACRITIC2NAME[text[i]])\n",
        "                temp = DIACRITIC2NAME[text[i]]\n",
        "                if (temp == 'Fatha'):\n",
        "                    classes.append(0)\n",
        "                elif (temp == 'Fathatan'):\n",
        "                    classes.append(1)\n",
        "                elif (temp == 'Damma'):\n",
        "                    classes.append(2)\n",
        "                elif (temp == 'Dammatan'):\n",
        "                    classes.append(3)\n",
        "                elif (temp == 'Kasra'):\n",
        "                    classes.append(4)\n",
        "                elif (temp == 'Kasratan'):\n",
        "                    classes.append(5)\n",
        "                elif (temp == 'Sukun'):\n",
        "                    classes.append(6)\n",
        "                elif (temp == 'Shadda'):\n",
        "                    classes.append(7)\n",
        "        elif text[i - 1] not in ARABIC_DIACRITICS:\n",
        "            diacritics.append('')\n",
        "            classes.append(14)\n",
        "\n",
        "    if text[-1] not in ARABIC_DIACRITICS:\n",
        "        diacritics.append('')\n",
        "        classes.append(14)\n",
        "    return diacritics, classes\n",
        "\n",
        "\n",
        "def extract_arabic_words2(text):\n",
        "    arabic_pattern = re.compile('[\\u0600-\\u06FF]+')\n",
        "    arabic_matches = arabic_pattern.findall(text)\n",
        "    result = ' '.join(arabic_matches)\n",
        "    processed_text = re.sub(r'[؛،\\.]+', '', result)\n",
        "    final_processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
        "    return final_processed_text\n",
        "\n",
        "\n",
        "input_file_path = \"train.txt\"  # Replace with your input file path\n",
        "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "    input_text = input_file.read()\n",
        "\n",
        "arabic_words = extract_arabic_words2(input_text)\n",
        "\n",
        "output_words = clear_diacritics(arabic_words)\n",
        "words = output_words.split()\n",
        "words2 = arabic_words.split()\n",
        "words_array = [list(word) for word in words]\n",
        "words_array2 = [list(word2) for word2 in words2]\n",
        "\n",
        "output_without_spaces = arabic_words.replace(\" \", \"\")\n",
        "output_without_spaces2 = output_words.replace(\" \", \"\")\n",
        "array_of_chars = [char for char in output_without_spaces]\n",
        "_,classes_extraction = extract_diacritics (output_without_spaces)\n",
        "\n",
        "\n",
        "num_feature = 30\n",
        "min_word_count = 1\n",
        "num_thread = 5\n",
        "window_size = 10\n",
        "down_sampling = 0.001\n",
        "iteration = 20\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "model_fastText = FastText(words_array,\n",
        "                        vector_size=num_feature,\n",
        "                        window=window_size,\n",
        "                        min_count=min_word_count,\n",
        "                        workers=num_thread)\n",
        "\n",
        "\n",
        "j=0\n",
        "chars =[]\n",
        "char_vectors =[]\n",
        "char_classes=[]\n",
        "for word in words_array:\n",
        "  for char in word:\n",
        "    chars.append(char)\n",
        "    char_classes.append(classes_extraction[j])\n",
        "    vector = model_fastText.wv[char]\n",
        "    char_vectors.append(vector)\n",
        "    j=j+1\n",
        "\n",
        "print (j)\n",
        "print(chars[1])\n",
        "print(char_classes[1])\n",
        "print(char_vectors[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZy2t_rZpNp",
        "outputId": "d10eb39b-ef53-44ad-f648-0ec5dad43092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8353805\n",
            "8353805\n",
            "[array([-0.59912956, -2.3143928 , -0.4928575 , -0.7636472 ,  2.5471454 ,\n",
            "        0.265863  ,  1.110589  , -0.86562574,  0.4167358 , -0.3797188 ],\n",
            "      dtype=float32), array([-0.05952365,  0.7864038 , -0.30884534,  0.00966583, -0.31647366,\n",
            "       -0.37679344,  0.14028403, -0.0656407 ,  0.3308646 ,  0.13131635],\n",
            "      dtype=float32), array([ 1.2460281 ,  0.07789117,  0.35825545,  1.0349609 , -0.27964464,\n",
            "       -0.87966335, -0.05130558, -0.42975643, -0.4579924 , -0.21386081],\n",
            "      dtype=float32), array([ 0.38726312, -0.2088649 , -0.9019669 , -0.8579454 , -0.9554811 ,\n",
            "       -0.12167442, -0.35786894,  0.23730293, -0.02445211,  0.2521064 ],\n",
            "      dtype=float32)]\n",
            "[0, 6, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "print(len(char_vectors))\n",
        "print(len(char_classes))\n",
        "print(char_vectors[:4])\n",
        "print(char_classes[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wUiTrqSAaH9V"
      },
      "outputs": [],
      "source": [
        "# prompt: generate pytorch class \"TashkelaSet\" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders\n",
        "\n",
        "\n",
        "# import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# # import tensorflow as tf\n",
        "# import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from datetime import datetime\n",
        "# from gensim.models import *\n",
        "# import logging\n",
        "\n",
        "class TashkelaSet(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "def prepare_data(train_X, train_y):\n",
        "\n",
        "  train_set = TashkelaSet(train_X, train_y)\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "\n",
        "  return train_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txsFRUhBYD50"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fl6RxXODYHQm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        #h0 dimesions should be 2-D\n",
        "        # h0 = h0.unsqueeze(0)\n",
        "        #x.size(-1) must be equal to input_size\n",
        "        x = x.unsqueeze(0)\n",
        "        # Forward pass through the RNN\n",
        "        out, hn = self.rnn(x, h0)\n",
        "\n",
        "        # Select the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hn\n",
        "\n",
        "#####################\n",
        "def train_model(model, train_loader):\n",
        "    \"\"\"\n",
        "    Function for training the model\n",
        "    \"\"\"\n",
        "    # define the optimization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    # define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # epochs\n",
        "    epochs = 10\n",
        "    # loop over the epochs\n",
        "    for epoch in range(epochs):\n",
        "        # initialize the hidden state\n",
        "        h0 = torch.zeros(1, 3, hidden_size)\n",
        "        # loop over the dataset\n",
        "        for inputs, labels in train_loader:\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, labels)\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "        # print the loss\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "#####################\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Function for evaluating the model\n",
        "    \"\"\"\n",
        "    # initialize the hidden state\n",
        "    h0 = torch.zeros(1, 3, hidden_size)\n",
        "    # initialize the accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # deactivating autograd\n",
        "    with torch.no_grad():\n",
        "        # loop over the test dataset\n",
        "        for inputs, labels in test_loader:\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # get predictions from the maximum value\n",
        "            _, predicted = torch.max(yhat.data, 1)\n",
        "            # update total\n",
        "            total += labels.size(0)\n",
        "            # update correct\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    # compute the accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    # print the accuracy\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XI165qYIco7x"
      },
      "outputs": [],
      "source": [
        "# prompt: now test the whole code in action , train , validate/evaluate and feel free to add necessary code , that name of the train file is train.txt  and validation set is val.txt\n",
        "\n",
        "# **Data Preparation**\n",
        "\n",
        "train_path = 'train.txt'\n",
        "val_path = 'val.txt'\n",
        "\n",
        "train_loader = prepare_data(char_vectors, char_classes)\n",
        "\n",
        "# **Model Definition**\n",
        "\n",
        "input_size = len(char_vectors)\n",
        "hidden_size = 128\n",
        "output_size = len(char_classes)\n",
        "\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# **Training**\n",
        "\n",
        "# train_model(model, train_loader)\n",
        "\n",
        "# **Evaluation**\n",
        "\n",
        "# evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oNV27l-9dA1d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "input.size(-1) must be equal to input_size. Expected 8353805, got 10",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-d7bfc291b6b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# **Training**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# **Evaluation**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-fdd62f31575f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m# compute the model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-fdd62f31575f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, h0)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Forward pass through the RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Select the output from the last time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mhx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RNN_TANH'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RNN_RELU'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    234\u001b[0m                 f'input must have {expected_input_dim} dimensions, got {input.dim()}')\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    237\u001b[0m                 f'input.size(-1) must be equal to input_size. Expected {self.input_size}, got {input.size(-1)}')\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 8353805, got 10"
          ]
        }
      ],
      "source": [
        "# **Training**\n",
        "\n",
        "train_model(model, train_loader)\n",
        "\n",
        "# **Evaluation**\n",
        "\n",
        "# evaluate_model(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 279152786208420 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-aa3f7455361b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-22-aa3f7455361b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min2hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min2output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 279152786208420 bytes."
          ]
        }
      ],
      "source": [
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x, hidden_state):\n",
        "        combined = torch.cat((x, hidden_state), 1)\n",
        "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
        "        output = self.in2output(combined)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
        "    \n",
        "hidden_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = MyRNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "print_interval = 3000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    random.shuffle(train_dataset)\n",
        "    for i, (name, label) in enumerate(train_dataset):\n",
        "        hidden_state = model.init_hidden()\n",
        "        for char in name:\n",
        "            output, hidden_state = model(char, hidden_state)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i + 1) % print_interval == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
