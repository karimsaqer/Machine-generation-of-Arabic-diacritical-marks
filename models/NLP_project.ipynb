{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nitRY9riXhIk",
        "outputId": "a5aac416-882f-448f-9c15-26024b7eebe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-01 04:17:19,232 : INFO : collecting all words and their counts\n",
            "2024-01-01 04:17:19,245 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2024-01-01 04:17:19,255 : INFO : PROGRESS: at sentence #10000, processed 39789 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,263 : INFO : PROGRESS: at sentence #20000, processed 79504 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,272 : INFO : PROGRESS: at sentence #30000, processed 119409 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,282 : INFO : PROGRESS: at sentence #40000, processed 159213 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,346 : INFO : PROGRESS: at sentence #50000, processed 199177 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,355 : INFO : PROGRESS: at sentence #60000, processed 238890 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,365 : INFO : PROGRESS: at sentence #70000, processed 278484 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,440 : INFO : PROGRESS: at sentence #80000, processed 318019 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,538 : INFO : PROGRESS: at sentence #90000, processed 358282 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,547 : INFO : PROGRESS: at sentence #100000, processed 397837 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,555 : INFO : PROGRESS: at sentence #110000, processed 437275 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,563 : INFO : PROGRESS: at sentence #120000, processed 477034 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,572 : INFO : PROGRESS: at sentence #130000, processed 516549 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,581 : INFO : PROGRESS: at sentence #140000, processed 556154 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,589 : INFO : PROGRESS: at sentence #150000, processed 595763 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,598 : INFO : PROGRESS: at sentence #160000, processed 635590 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,606 : INFO : PROGRESS: at sentence #170000, processed 675078 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,615 : INFO : PROGRESS: at sentence #180000, processed 714627 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,625 : INFO : PROGRESS: at sentence #190000, processed 754301 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,640 : INFO : PROGRESS: at sentence #200000, processed 794082 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,652 : INFO : PROGRESS: at sentence #210000, processed 833832 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,661 : INFO : PROGRESS: at sentence #220000, processed 873657 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,669 : INFO : PROGRESS: at sentence #230000, processed 913559 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,680 : INFO : PROGRESS: at sentence #240000, processed 952887 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,690 : INFO : PROGRESS: at sentence #250000, processed 992887 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,698 : INFO : PROGRESS: at sentence #260000, processed 1033054 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,707 : INFO : PROGRESS: at sentence #270000, processed 1072345 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,717 : INFO : PROGRESS: at sentence #280000, processed 1111668 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,726 : INFO : PROGRESS: at sentence #290000, processed 1151232 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,735 : INFO : PROGRESS: at sentence #300000, processed 1190646 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,743 : INFO : PROGRESS: at sentence #310000, processed 1230502 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,751 : INFO : PROGRESS: at sentence #320000, processed 1269998 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,760 : INFO : PROGRESS: at sentence #330000, processed 1309855 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,769 : INFO : PROGRESS: at sentence #340000, processed 1349800 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,778 : INFO : PROGRESS: at sentence #350000, processed 1389345 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,788 : INFO : PROGRESS: at sentence #360000, processed 1428928 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,798 : INFO : PROGRESS: at sentence #370000, processed 1468377 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,805 : INFO : PROGRESS: at sentence #380000, processed 1508164 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,820 : INFO : PROGRESS: at sentence #390000, processed 1548218 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,830 : INFO : PROGRESS: at sentence #400000, processed 1587817 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,841 : INFO : PROGRESS: at sentence #410000, processed 1627522 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,849 : INFO : PROGRESS: at sentence #420000, processed 1666992 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,860 : INFO : PROGRESS: at sentence #430000, processed 1706547 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,871 : INFO : PROGRESS: at sentence #440000, processed 1746374 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,882 : INFO : PROGRESS: at sentence #450000, processed 1786268 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,890 : INFO : PROGRESS: at sentence #460000, processed 1826021 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,902 : INFO : PROGRESS: at sentence #470000, processed 1865512 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,914 : INFO : PROGRESS: at sentence #480000, processed 1905221 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,923 : INFO : PROGRESS: at sentence #490000, processed 1944821 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,932 : INFO : PROGRESS: at sentence #500000, processed 1984798 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,940 : INFO : PROGRESS: at sentence #510000, processed 2024679 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,950 : INFO : PROGRESS: at sentence #520000, processed 2064741 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,961 : INFO : PROGRESS: at sentence #530000, processed 2104349 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,970 : INFO : PROGRESS: at sentence #540000, processed 2143791 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,979 : INFO : PROGRESS: at sentence #550000, processed 2183137 words, keeping 37 word types\n",
            "2024-01-01 04:17:19,990 : INFO : PROGRESS: at sentence #560000, processed 2222910 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,000 : INFO : PROGRESS: at sentence #570000, processed 2262661 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,013 : INFO : PROGRESS: at sentence #580000, processed 2301990 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,022 : INFO : PROGRESS: at sentence #590000, processed 2341673 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,033 : INFO : PROGRESS: at sentence #600000, processed 2381194 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,042 : INFO : PROGRESS: at sentence #610000, processed 2420633 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,052 : INFO : PROGRESS: at sentence #620000, processed 2460029 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,064 : INFO : PROGRESS: at sentence #630000, processed 2499926 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,081 : INFO : PROGRESS: at sentence #640000, processed 2539803 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,101 : INFO : PROGRESS: at sentence #650000, processed 2579565 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,118 : INFO : PROGRESS: at sentence #660000, processed 2619300 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,130 : INFO : PROGRESS: at sentence #670000, processed 2659180 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,140 : INFO : PROGRESS: at sentence #680000, processed 2698895 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,150 : INFO : PROGRESS: at sentence #690000, processed 2738873 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,159 : INFO : PROGRESS: at sentence #700000, processed 2778101 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,170 : INFO : PROGRESS: at sentence #710000, processed 2817552 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,179 : INFO : PROGRESS: at sentence #720000, processed 2856924 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,189 : INFO : PROGRESS: at sentence #730000, processed 2896457 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,198 : INFO : PROGRESS: at sentence #740000, processed 2936062 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,208 : INFO : PROGRESS: at sentence #750000, processed 2975628 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,219 : INFO : PROGRESS: at sentence #760000, processed 3015326 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,258 : INFO : PROGRESS: at sentence #770000, processed 3054984 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,302 : INFO : PROGRESS: at sentence #780000, processed 3094678 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,325 : INFO : PROGRESS: at sentence #790000, processed 3134247 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,335 : INFO : PROGRESS: at sentence #800000, processed 3173415 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,350 : INFO : PROGRESS: at sentence #810000, processed 3213036 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,360 : INFO : PROGRESS: at sentence #820000, processed 3252596 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,368 : INFO : PROGRESS: at sentence #830000, processed 3292479 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,379 : INFO : PROGRESS: at sentence #840000, processed 3332144 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,395 : INFO : PROGRESS: at sentence #850000, processed 3371890 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,406 : INFO : PROGRESS: at sentence #860000, processed 3411405 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,417 : INFO : PROGRESS: at sentence #870000, processed 3450922 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,430 : INFO : PROGRESS: at sentence #880000, processed 3490273 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,439 : INFO : PROGRESS: at sentence #890000, processed 3529901 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,449 : INFO : PROGRESS: at sentence #900000, processed 3569633 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,473 : INFO : PROGRESS: at sentence #910000, processed 3609405 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,484 : INFO : PROGRESS: at sentence #920000, processed 3649070 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,502 : INFO : PROGRESS: at sentence #930000, processed 3688835 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,511 : INFO : PROGRESS: at sentence #940000, processed 3729028 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,521 : INFO : PROGRESS: at sentence #950000, processed 3768766 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,530 : INFO : PROGRESS: at sentence #960000, processed 3808515 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,545 : INFO : PROGRESS: at sentence #970000, processed 3848122 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,553 : INFO : PROGRESS: at sentence #980000, processed 3888156 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,565 : INFO : PROGRESS: at sentence #990000, processed 3927889 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,579 : INFO : PROGRESS: at sentence #1000000, processed 3967805 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,589 : INFO : PROGRESS: at sentence #1010000, processed 4007568 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,599 : INFO : PROGRESS: at sentence #1020000, processed 4047032 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,609 : INFO : PROGRESS: at sentence #1030000, processed 4086707 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,620 : INFO : PROGRESS: at sentence #1040000, processed 4126677 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,631 : INFO : PROGRESS: at sentence #1050000, processed 4166867 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,639 : INFO : PROGRESS: at sentence #1060000, processed 4206076 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,649 : INFO : PROGRESS: at sentence #1070000, processed 4245586 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,662 : INFO : PROGRESS: at sentence #1080000, processed 4285134 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,673 : INFO : PROGRESS: at sentence #1090000, processed 4325104 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,682 : INFO : PROGRESS: at sentence #1100000, processed 4364839 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,691 : INFO : PROGRESS: at sentence #1110000, processed 4404376 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,700 : INFO : PROGRESS: at sentence #1120000, processed 4444283 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,710 : INFO : PROGRESS: at sentence #1130000, processed 4484058 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,720 : INFO : PROGRESS: at sentence #1140000, processed 4523533 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,729 : INFO : PROGRESS: at sentence #1150000, processed 4563133 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,737 : INFO : PROGRESS: at sentence #1160000, processed 4603000 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,751 : INFO : PROGRESS: at sentence #1170000, processed 4642804 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,762 : INFO : PROGRESS: at sentence #1180000, processed 4682511 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,771 : INFO : PROGRESS: at sentence #1190000, processed 4722216 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,780 : INFO : PROGRESS: at sentence #1200000, processed 4762313 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,791 : INFO : PROGRESS: at sentence #1210000, processed 4801723 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,800 : INFO : PROGRESS: at sentence #1220000, processed 4841704 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,812 : INFO : PROGRESS: at sentence #1230000, processed 4881737 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,822 : INFO : PROGRESS: at sentence #1240000, processed 4921272 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,831 : INFO : PROGRESS: at sentence #1250000, processed 4961237 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,845 : INFO : PROGRESS: at sentence #1260000, processed 5000924 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,857 : INFO : PROGRESS: at sentence #1270000, processed 5040594 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,865 : INFO : PROGRESS: at sentence #1280000, processed 5080123 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,874 : INFO : PROGRESS: at sentence #1290000, processed 5120057 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,883 : INFO : PROGRESS: at sentence #1300000, processed 5160129 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,893 : INFO : PROGRESS: at sentence #1310000, processed 5199882 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,902 : INFO : PROGRESS: at sentence #1320000, processed 5239771 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,913 : INFO : PROGRESS: at sentence #1330000, processed 5279659 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,921 : INFO : PROGRESS: at sentence #1340000, processed 5319292 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,934 : INFO : PROGRESS: at sentence #1350000, processed 5359138 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,947 : INFO : PROGRESS: at sentence #1360000, processed 5398630 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,959 : INFO : PROGRESS: at sentence #1370000, processed 5438354 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,968 : INFO : PROGRESS: at sentence #1380000, processed 5478486 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,977 : INFO : PROGRESS: at sentence #1390000, processed 5517952 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,987 : INFO : PROGRESS: at sentence #1400000, processed 5557567 words, keeping 37 word types\n",
            "2024-01-01 04:17:20,995 : INFO : PROGRESS: at sentence #1410000, processed 5597301 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,007 : INFO : PROGRESS: at sentence #1420000, processed 5636809 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,017 : INFO : PROGRESS: at sentence #1430000, processed 5676528 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,041 : INFO : PROGRESS: at sentence #1440000, processed 5716042 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,066 : INFO : PROGRESS: at sentence #1450000, processed 5755917 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,096 : INFO : PROGRESS: at sentence #1460000, processed 5795331 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,113 : INFO : PROGRESS: at sentence #1470000, processed 5835086 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,123 : INFO : PROGRESS: at sentence #1480000, processed 5874838 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,139 : INFO : PROGRESS: at sentence #1490000, processed 5914415 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,154 : INFO : PROGRESS: at sentence #1500000, processed 5954294 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,166 : INFO : PROGRESS: at sentence #1510000, processed 5994116 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,175 : INFO : PROGRESS: at sentence #1520000, processed 6033660 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,185 : INFO : PROGRESS: at sentence #1530000, processed 6073293 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,194 : INFO : PROGRESS: at sentence #1540000, processed 6113160 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,206 : INFO : PROGRESS: at sentence #1550000, processed 6152725 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,216 : INFO : PROGRESS: at sentence #1560000, processed 6192512 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,226 : INFO : PROGRESS: at sentence #1570000, processed 6231998 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,236 : INFO : PROGRESS: at sentence #1580000, processed 6271740 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,249 : INFO : PROGRESS: at sentence #1590000, processed 6311521 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,260 : INFO : PROGRESS: at sentence #1600000, processed 6351673 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,270 : INFO : PROGRESS: at sentence #1610000, processed 6391580 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,279 : INFO : PROGRESS: at sentence #1620000, processed 6431140 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,290 : INFO : PROGRESS: at sentence #1630000, processed 6470506 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,300 : INFO : PROGRESS: at sentence #1640000, processed 6510145 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,309 : INFO : PROGRESS: at sentence #1650000, processed 6550219 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,319 : INFO : PROGRESS: at sentence #1660000, processed 6589881 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,332 : INFO : PROGRESS: at sentence #1670000, processed 6629610 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,341 : INFO : PROGRESS: at sentence #1680000, processed 6669268 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,349 : INFO : PROGRESS: at sentence #1690000, processed 6709023 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,359 : INFO : PROGRESS: at sentence #1700000, processed 6748647 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,371 : INFO : PROGRESS: at sentence #1710000, processed 6788198 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,384 : INFO : PROGRESS: at sentence #1720000, processed 6828244 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,393 : INFO : PROGRESS: at sentence #1730000, processed 6868399 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,402 : INFO : PROGRESS: at sentence #1740000, processed 6907915 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,414 : INFO : PROGRESS: at sentence #1750000, processed 6947456 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,423 : INFO : PROGRESS: at sentence #1760000, processed 6986928 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,433 : INFO : PROGRESS: at sentence #1770000, processed 7026801 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,442 : INFO : PROGRESS: at sentence #1780000, processed 7066400 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,452 : INFO : PROGRESS: at sentence #1790000, processed 7106028 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,469 : INFO : PROGRESS: at sentence #1800000, processed 7145639 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,482 : INFO : PROGRESS: at sentence #1810000, processed 7185106 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,494 : INFO : PROGRESS: at sentence #1820000, processed 7224781 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,503 : INFO : PROGRESS: at sentence #1830000, processed 7264511 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,514 : INFO : PROGRESS: at sentence #1840000, processed 7304089 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,523 : INFO : PROGRESS: at sentence #1850000, processed 7343290 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,533 : INFO : PROGRESS: at sentence #1860000, processed 7383079 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,543 : INFO : PROGRESS: at sentence #1870000, processed 7423161 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,552 : INFO : PROGRESS: at sentence #1880000, processed 7462753 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,562 : INFO : PROGRESS: at sentence #1890000, processed 7502483 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,573 : INFO : PROGRESS: at sentence #1900000, processed 7542173 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,582 : INFO : PROGRESS: at sentence #1910000, processed 7581763 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,593 : INFO : PROGRESS: at sentence #1920000, processed 7621382 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,604 : INFO : PROGRESS: at sentence #1930000, processed 7661456 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,619 : INFO : PROGRESS: at sentence #1940000, processed 7701323 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,633 : INFO : PROGRESS: at sentence #1950000, processed 7740868 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,648 : INFO : PROGRESS: at sentence #1960000, processed 7780679 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,669 : INFO : PROGRESS: at sentence #1970000, processed 7820265 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,680 : INFO : PROGRESS: at sentence #1980000, processed 7859691 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,691 : INFO : PROGRESS: at sentence #1990000, processed 7899377 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,700 : INFO : PROGRESS: at sentence #2000000, processed 7938930 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,711 : INFO : PROGRESS: at sentence #2010000, processed 7978691 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,720 : INFO : PROGRESS: at sentence #2020000, processed 8018714 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,730 : INFO : PROGRESS: at sentence #2030000, processed 8058366 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,739 : INFO : PROGRESS: at sentence #2040000, processed 8098031 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,751 : INFO : PROGRESS: at sentence #2050000, processed 8137699 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,767 : INFO : PROGRESS: at sentence #2060000, processed 8177398 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,783 : INFO : PROGRESS: at sentence #2070000, processed 8217354 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,795 : INFO : PROGRESS: at sentence #2080000, processed 8257308 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,804 : INFO : PROGRESS: at sentence #2090000, processed 8296798 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,813 : INFO : PROGRESS: at sentence #2100000, processed 8336716 words, keeping 37 word types\n",
            "2024-01-01 04:17:21,818 : INFO : collected 37 word types from a corpus of 8353805 raw words and 2104234 sentences\n",
            "2024-01-01 04:17:21,819 : INFO : Creating a fresh vocabulary\n",
            "2024-01-01 04:17:21,851 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 37 unique words (100.00% of original 37, drops 0)', 'datetime': '2024-01-01T04:17:21.850111', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 04:17:21,853 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 8353805 word corpus (100.00% of original 8353805, drops 0)', 'datetime': '2024-01-01T04:17:21.853056', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 04:17:21,933 : INFO : deleting the raw counts dictionary of 37 items\n",
            "2024-01-01 04:17:21,936 : INFO : sample=0.001 downsamples 32 most-common words\n",
            "2024-01-01 04:17:21,939 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 1669253.6281965326 word corpus (20.0%% of prior 8353805)', 'datetime': '2024-01-01T04:17:21.939762', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
            "2024-01-01 04:17:22,013 : INFO : estimated required memory for 37 words, 2000000 buckets and 30 dimensions: 240031292 bytes\n",
            "2024-01-01 04:17:22,015 : INFO : resetting layer weights\n",
            "2024-01-01 04:17:22,396 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-01T04:17:22.396114', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
            "2024-01-01 04:17:22,397 : INFO : FastText lifecycle event {'msg': 'training model with 5 workers on 37 vocabulary and 30 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-01-01T04:17:22.397113', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
            "2024-01-01 04:17:23,745 : INFO : EPOCH 0 - PROGRESS: at 21.19% examples, 351877 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 04:17:24,750 : INFO : EPOCH 0 - PROGRESS: at 57.59% examples, 478297 words/s, in_qsize 10, out_qsize 0\n",
            "2024-01-01 04:17:25,752 : INFO : EPOCH 0 - PROGRESS: at 92.76% examples, 513995 words/s, in_qsize 9, out_qsize 1\n",
            "2024-01-01 04:17:25,927 : INFO : EPOCH 0: training on 8353805 raw words (1669867 effective words) took 3.2s, 523889 effective words/s\n",
            "2024-01-01 04:17:27,048 : INFO : EPOCH 1 - PROGRESS: at 31.97% examples, 528124 words/s, in_qsize 8, out_qsize 2\n",
            "2024-01-01 04:17:28,052 : INFO : EPOCH 1 - PROGRESS: at 68.59% examples, 568202 words/s, in_qsize 9, out_qsize 0\n",
            "2024-01-01 04:17:28,946 : INFO : EPOCH 1: training on 8353805 raw words (1669217 effective words) took 2.9s, 573784 effective words/s\n",
            "2024-01-01 04:17:29,965 : INFO : EPOCH 2 - PROGRESS: at 34.97% examples, 580068 words/s, in_qsize 8, out_qsize 2\n",
            "2024-01-01 04:17:30,968 : INFO : EPOCH 2 - PROGRESS: at 70.63% examples, 586856 words/s, in_qsize 9, out_qsize 1\n",
            "2024-01-01 04:17:31,940 : INFO : EPOCH 2: training on 8353805 raw words (1668908 effective words) took 3.0s, 559879 effective words/s\n",
            "2024-01-01 04:17:33,281 : INFO : EPOCH 3 - PROGRESS: at 35.93% examples, 599052 words/s, in_qsize 10, out_qsize 0\n",
            "2024-01-01 04:17:34,287 : INFO : EPOCH 3 - PROGRESS: at 70.62% examples, 586837 words/s, in_qsize 8, out_qsize 1\n",
            "2024-01-01 04:17:35,076 : INFO : EPOCH 3: training on 8353805 raw words (1668528 effective words) took 2.8s, 596585 effective words/s\n",
            "2024-01-01 04:17:36,099 : INFO : EPOCH 4 - PROGRESS: at 34.97% examples, 580691 words/s, in_qsize 10, out_qsize 0\n",
            "2024-01-01 04:17:37,103 : INFO : EPOCH 4 - PROGRESS: at 68.35% examples, 567870 words/s, in_qsize 10, out_qsize 1\n",
            "2024-01-01 04:17:37,958 : INFO : EPOCH 4: training on 8353805 raw words (1668022 effective words) took 2.9s, 582752 effective words/s\n",
            "2024-01-01 04:17:37,959 : INFO : FastText lifecycle event {'msg': 'training on 41769025 raw words (8344542 effective words) took 15.6s, 536224 effective words/s', 'datetime': '2024-01-01T04:17:37.959015', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
            "2024-01-01 04:17:37,960 : INFO : FastText lifecycle event {'params': 'FastText<vocab=37, vector_size=30, alpha=0.025>', 'datetime': '2024-01-01T04:17:37.960012', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8353805\n",
            "و\n",
            "6\n",
            "[ 0.62478596 -0.54484314 -0.24169168  0.32632577 -0.16242056  0.56999254\n",
            " -0.28666466 -0.18571904 -0.05786865 -0.5390593   0.03933846 -0.05763604\n",
            "  0.00217669  0.16473043 -0.39401725  0.57397413  0.39120913 -0.06798539\n",
            " -0.27908233 -0.00995119  0.05086441  0.30112112 -0.30840826 -0.02778701\n",
            " -0.17920655 -0.12461371 -0.37938774 -0.0693948   0.21793753  0.17531076]\n"
          ]
        }
      ],
      "source": [
        "# %pip uninstall tensorflow\n",
        "# %pip install tensorflow\n",
        "# %pip install keras\n",
        "# %pip install gensim\n",
        "# %pip install nltk\n",
        "# %pip install torch\n",
        "# %pip install fasttext\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "from datetime import datetime\n",
        "from gensim.models import *\n",
        "import logging\n",
        "import fasttext\n",
        "# from rnn_utils import *\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "''' D_NAMES: This is a list containing names of various Arabic diacritics. Each\n",
        " element of the list represents a specific diacritic type. '''\n",
        "D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping\n",
        "from diacritic names to their corresponding Unicode characters.'''\n",
        "NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''\n",
        "DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode\n",
        " characters of all the diacritics.'''\n",
        "ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())\n",
        "\n",
        "\n",
        "# Remove all standard diacritics from the text, leaving the letters only.\n",
        "def clear_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    return ''.join([l for l in text if l not in ARABIC_DIACRITICS])\n",
        "\n",
        "\n",
        "# Return the diacritics from the text while keeping their original positions.\n",
        "def extract_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    diacritics = []\n",
        "    classes = []\n",
        "    temp = ''\n",
        "    for i in range(1, len(text)):\n",
        "        temp = ''\n",
        "        if text[i] in ARABIC_DIACRITICS:\n",
        "            if text[i-1] == NAME2DIACRITIC['Shadda']:\n",
        "                diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                if (temp == ('Shadda', 'Fatha')):\n",
        "                    classes.pop()\n",
        "                    classes.append(8)\n",
        "                elif (temp == ('Shadda', 'Fathatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(9)\n",
        "                elif (temp == ('Shadda', 'Damma')):\n",
        "                    classes.pop()\n",
        "                    classes.append(10)\n",
        "                elif (temp == ('Shadda', 'Dammatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(11)\n",
        "                elif (temp == ('Shadda', 'Kasra')):\n",
        "                    classes.pop()\n",
        "                    classes.append(12)\n",
        "                elif (temp == ('Shadda', 'Kasratan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(13)\n",
        "            else:\n",
        "                diacritics.append(DIACRITIC2NAME[text[i]])\n",
        "                temp = DIACRITIC2NAME[text[i]]\n",
        "                if (temp == 'Fatha'):\n",
        "                    classes.append(0)\n",
        "                elif (temp == 'Fathatan'):\n",
        "                    classes.append(1)\n",
        "                elif (temp == 'Damma'):\n",
        "                    classes.append(2)\n",
        "                elif (temp == 'Dammatan'):\n",
        "                    classes.append(3)\n",
        "                elif (temp == 'Kasra'):\n",
        "                    classes.append(4)\n",
        "                elif (temp == 'Kasratan'):\n",
        "                    classes.append(5)\n",
        "                elif (temp == 'Sukun'):\n",
        "                    classes.append(6)\n",
        "                elif (temp == 'Shadda'):\n",
        "                    classes.append(7)\n",
        "        elif text[i - 1] not in ARABIC_DIACRITICS:\n",
        "            diacritics.append('')\n",
        "            classes.append(14)\n",
        "\n",
        "    if text[-1] not in ARABIC_DIACRITICS:\n",
        "        diacritics.append('')\n",
        "        classes.append(14)\n",
        "    return diacritics, classes\n",
        "\n",
        "\n",
        "def extract_arabic_words2(text):\n",
        "    arabic_pattern = re.compile('[\\u0600-\\u06FF]+')\n",
        "    arabic_matches = arabic_pattern.findall(text)\n",
        "    result = ' '.join(arabic_matches)\n",
        "    processed_text = re.sub(r'[؛،\\.]+', '', result)\n",
        "    final_processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
        "    return final_processed_text\n",
        "\n",
        "\n",
        "input_file_path = \"train.txt\"  # Replace with your input file path\n",
        "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "    input_text = input_file.read()\n",
        "\n",
        "arabic_words = extract_arabic_words2(input_text)\n",
        "\n",
        "output_words = clear_diacritics(arabic_words)\n",
        "words = output_words.split()\n",
        "words2 = arabic_words.split()\n",
        "words_array = [list(word) for word in words]\n",
        "words_array2 = [list(word2) for word2 in words2]\n",
        "\n",
        "output_without_spaces = arabic_words.replace(\" \", \"\")\n",
        "output_without_spaces2 = output_words.replace(\" \", \"\")\n",
        "array_of_chars = [char for char in output_without_spaces]\n",
        "_,classes_extraction = extract_diacritics (output_without_spaces)\n",
        "\n",
        "\n",
        "num_feature = 30\n",
        "min_word_count = 1\n",
        "num_thread = 5\n",
        "window_size = 10\n",
        "down_sampling = 0.001\n",
        "iteration = 20\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "model_fastText = FastText(words_array,\n",
        "                        vector_size=num_feature,\n",
        "                        window=window_size,\n",
        "                        min_count=min_word_count,\n",
        "                        workers=num_thread)\n",
        "\n",
        "\n",
        "j=0\n",
        "chars =[]\n",
        "char_vectors =[]\n",
        "char_classes=[]\n",
        "for word in words_array:\n",
        "  for char in word:\n",
        "    chars.append(char)\n",
        "    char_classes.append(classes_extraction[j])\n",
        "    vector = model_fastText.wv[char]\n",
        "    char_vectors.append(vector)\n",
        "    j=j+1\n",
        "\n",
        "print (j)\n",
        "print(chars[1])\n",
        "print(char_classes[1])\n",
        "print(char_vectors[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZy2t_rZpNp",
        "outputId": "d10eb39b-ef53-44ad-f648-0ec5dad43092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8353805\n",
            "8353805\n",
            "[array([ 1.3533134 ,  0.625608  ,  0.09520277, -0.7149362 ,  2.2271712 ,\n",
            "        0.29656577,  0.38094717,  0.2807173 ,  0.64498925, -1.2605476 ,\n",
            "       -0.97702724, -0.27355883,  0.62604344, -0.5280238 ,  0.35345432,\n",
            "       -0.88688296, -1.1184022 , -0.3872087 , -0.04730593, -0.01448194,\n",
            "       -0.08826903, -1.3650999 ,  0.6777671 ,  0.67260814,  0.64454854,\n",
            "        1.0960552 ,  0.10407556, -0.93675077, -0.3922763 , -0.15128514],\n",
            "      dtype=float32), array([ 0.62478596, -0.54484314, -0.24169168,  0.32632577, -0.16242056,\n",
            "        0.56999254, -0.28666466, -0.18571904, -0.05786865, -0.5390593 ,\n",
            "        0.03933846, -0.05763604,  0.00217669,  0.16473043, -0.39401725,\n",
            "        0.57397413,  0.39120913, -0.06798539, -0.27908233, -0.00995119,\n",
            "        0.05086441,  0.30112112, -0.30840826, -0.02778701, -0.17920655,\n",
            "       -0.12461371, -0.37938774, -0.0693948 ,  0.21793753,  0.17531076],\n",
            "      dtype=float32), array([-0.07214908, -0.5860003 , -0.6406783 , -0.26651567,  0.26544073,\n",
            "       -0.4622618 ,  0.18729153, -0.32427692,  0.0469379 ,  0.5145677 ,\n",
            "       -0.1574751 , -0.08326884,  0.01207723, -0.13456687, -0.14457911,\n",
            "       -0.02239152, -0.26014334,  0.66985995, -0.43689358, -0.03071869,\n",
            "       -0.15867497, -0.4127931 , -0.24061498, -1.1768361 , -0.5686053 ,\n",
            "       -0.15699872,  0.4901407 , -0.42083225, -0.5092869 ,  0.5492767 ],\n",
            "      dtype=float32), array([-0.70305765,  0.614831  ,  0.82957643, -0.3792147 , -0.7903271 ,\n",
            "       -0.19347961,  0.25742385,  0.43444282,  0.6059655 ,  0.41843873,\n",
            "        0.7328866 ,  0.08887547,  1.0338182 , -0.05895358,  0.2251727 ,\n",
            "        0.2933577 ,  0.5751087 , -0.01018322, -0.7393396 ,  0.11322594,\n",
            "       -0.7746129 , -0.23064235,  0.54067814, -0.33641008, -0.7226698 ,\n",
            "        0.20573199, -0.20173043,  0.38883072,  0.05271867, -0.5984146 ],\n",
            "      dtype=float32)]\n",
            "[0, 6, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "print(len(char_vectors))\n",
        "print(len(char_classes))\n",
        "print(char_vectors[:4])\n",
        "print(char_classes[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wUiTrqSAaH9V"
      },
      "outputs": [],
      "source": [
        "# prompt: generate pytorch class \"TashkelaSet\" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders\n",
        "\n",
        "\n",
        "# import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# # import tensorflow as tf\n",
        "# import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from datetime import datetime\n",
        "# from gensim.models import *\n",
        "# import logging\n",
        "\n",
        "class TashkelaSet(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "def prepare_data(train_X, train_y):\n",
        "\n",
        "  train_set = TashkelaSet(train_X, train_y)\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "\n",
        "  return train_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txsFRUhBYD50"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fl6RxXODYHQm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        #h0 dimesions should be 2-D\n",
        "        h0 = h0.unsqueeze(0)\n",
        "        #x.size(-1) must be equal to input_size\n",
        "        x = x.unsqueeze(0)\n",
        "        # Forward pass through the RNN\n",
        "        out, hn = self.rnn(x, h0)\n",
        "\n",
        "        # Select the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hn\n",
        "\n",
        "#####################\n",
        "def train_model(model, train_loader):\n",
        "    \"\"\"\n",
        "    Function for training the model\n",
        "    \"\"\"\n",
        "    # define the optimization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    # define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # epochs\n",
        "    epochs = 10\n",
        "    # loop over the epochs\n",
        "    for epoch in range(epochs):\n",
        "        # initialize the hidden state\n",
        "        h0 = torch.zeros(1, 3, hidden_size)\n",
        "        # loop over the dataset\n",
        "        for inputs, labels in train_loader:\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, labels)\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "        # print the loss\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "#####################\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Function for evaluating the model\n",
        "    \"\"\"\n",
        "    # initialize the hidden state\n",
        "    h0 = torch.zeros(1, 3, hidden_size)\n",
        "    # initialize the accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # deactivating autograd\n",
        "    with torch.no_grad():\n",
        "        # loop over the test dataset\n",
        "        for inputs, labels in test_loader:\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # get predictions from the maximum value\n",
        "            _, predicted = torch.max(yhat.data, 1)\n",
        "            # update total\n",
        "            total += labels.size(0)\n",
        "            # update correct\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    # compute the accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    # print the accuracy\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XI165qYIco7x"
      },
      "outputs": [],
      "source": [
        "# prompt: now test the whole code in action , train , validate/evaluate and feel free to add necessary code , that name of the train file is train.txt  and validation set is val.txt\n",
        "\n",
        "# **Data Preparation**\n",
        "\n",
        "train_path = 'train.txt'\n",
        "val_path = 'val.txt'\n",
        "\n",
        "train_loader = prepare_data(char_vectors, char_classes)\n",
        "\n",
        "# **Model Definition**\n",
        "\n",
        "input_size = len(char_vectors)\n",
        "hidden_size = 128\n",
        "output_size = len(char_classes)\n",
        "\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# **Training**\n",
        "\n",
        "# train_model(model, train_loader)\n",
        "\n",
        "# **Evaluation**\n",
        "\n",
        "# evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oNV27l-9dA1d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "For batched 3-D input, hx should also be 3-D but got 4-D tensor",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-21-d7bfc291b6b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# **Training**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# **Evaluation**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-19-48829c18b781>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m# compute the model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-19-48829c18b781>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, h0)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Forward pass through the RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Select the output from the last time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m                     raise RuntimeError(\n\u001b[0m\u001b[0;32m    535\u001b[0m                         f\"For batched 3-D input, hx should also be 3-D but got {hx.dim()}-D tensor\")\n\u001b[0;32m    536\u001b[0m             \u001b[0mmax_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: For batched 3-D input, hx should also be 3-D but got 4-D tensor"
          ]
        }
      ],
      "source": [
        "# **Training**\n",
        "\n",
        "train_model(model, train_loader)\n",
        "\n",
        "# **Evaluation**\n",
        "\n",
        "# evaluate_model(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 279152786208420 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-aa3f7455361b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-22-aa3f7455361b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min2hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min2output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 279152786208420 bytes."
          ]
        }
      ],
      "source": [
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x, hidden_state):\n",
        "        combined = torch.cat((x, hidden_state), 1)\n",
        "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
        "        output = self.in2output(combined)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
        "    \n",
        "hidden_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = MyRNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "print_interval = 3000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    random.shuffle(train_dataset)\n",
        "    for i, (name, label) in enumerate(train_dataset):\n",
        "        hidden_state = model.init_hidden()\n",
        "        for char in name:\n",
        "            output, hidden_state = model(char, hidden_state)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i + 1) % print_interval == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
