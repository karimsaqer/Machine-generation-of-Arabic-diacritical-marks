{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN1\n",
    "\n",
    "**Input:** Matrix of Word embeddings -> each element is an embedding for a word\n",
    "\n",
    "**Output:** Single final output at the end of the network -> we can call it the global context vector (can test GloVe method and get it ready as another alternative function for Word2Vec), we will call it h-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN2  \n",
    "#### ( input X:the sequence vector of characters X2 -> each element is a character)  \n",
    "(hidden input h-1 : is the output of RNN1)\n",
    "|  (output  vector of labels , Yi corresponds as a tashkela to Xi character\n",
    "\n",
    "**Input:** Matrix of Character embeddings -> each element is an embedding for a character\n",
    "\n",
    "**Output:** Matrix of labels -> each element is a label for a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lap2\\anaconda3\\lib\\site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipympl 0.9.3 requires matplotlib<4,>=3.4.0, but you have matplotlib 3.3.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lap2\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (20.9)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: keras in c:\\users\\lap2\\anaconda3\\lib\\site-packages (2.13.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\lap2\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lap2\\anaconda3\\lib\\site-packages (from gensim) (6.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: <built-in method __contains__ of dict object at 0x00000220C41A6F80> returned a result with an error set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-82b1ab842833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_TF_SetTarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_TF_SetConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: initialization failed"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "# %pip install numpy --upgrade\n",
    "%pip install numpy --upgrade --ignore-installed\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip install gensim\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from datetime import datetime\n",
    "from gensim.models import *\n",
    "import logging\n",
    "# from rnn_utils import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "''' D_NAMES: This is a list containing names of various Arabic diacritics. Each\n",
    " element of the list represents a specific diacritic type. '''\n",
    "D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping \n",
    "from diacritic names to their corresponding Unicode characters.'''\n",
    "NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''\n",
    "DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode\n",
    " characters of all the diacritics.'''\n",
    "ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())\n",
    "\n",
    "\n",
    "# Remove all standard diacritics from the text, leaving the letters only.\n",
    "def clear_diacritics(text):\n",
    "    assert isinstance(text, str)\n",
    "    return ''.join([l for l in text if l not in ARABIC_DIACRITICS])\n",
    "\n",
    "\n",
    "# Return the diacritics from the text while keeping their original positions.\n",
    "def extract_diacritics(text):\n",
    "    assert isinstance(text, str)\n",
    "    diacritics = []\n",
    "    classes = []\n",
    "    temp = ''\n",
    "    for i in range(1, len(text)):\n",
    "        temp = ''\n",
    "        if text[i] in ARABIC_DIACRITICS:\n",
    "            if text[i-1] == NAME2DIACRITIC['Shadda']:\n",
    "                diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
    "                temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
    "                if (temp == ('Shadda', 'Fatha')):\n",
    "                    classes.pop()\n",
    "                    classes.append(8)\n",
    "                elif (temp == ('Shadda', 'Fathatan')):\n",
    "                    classes.pop()\n",
    "                    classes.append(9)\n",
    "                elif (temp == ('Shadda', 'Damma')):\n",
    "                    classes.pop()\n",
    "                    classes.append(10)\n",
    "                elif (temp == ('Shadda', 'Dammatan')):\n",
    "                    classes.pop()\n",
    "                    classes.append(11)\n",
    "                elif (temp == ('Shadda', 'Kasra')):\n",
    "                    classes.pop()\n",
    "                    classes.append(12)\n",
    "                elif (temp == ('Shadda', 'Kasratan')):\n",
    "                    classes.pop()\n",
    "                    classes.append(13)\n",
    "            else:\n",
    "                diacritics.append(DIACRITIC2NAME[text[i]])\n",
    "                temp = DIACRITIC2NAME[text[i]]\n",
    "                if (temp == 'Fatha'):\n",
    "                    classes.append(0)\n",
    "                elif (temp == 'Fathatan'):\n",
    "                    classes.append(1)\n",
    "                elif (temp == 'Damma'):\n",
    "                    classes.append(2)\n",
    "                elif (temp == 'Dammatan'):\n",
    "                    classes.append(3)\n",
    "                elif (temp == 'Kasra'):\n",
    "                    classes.append(4)\n",
    "                elif (temp == 'Kasratan'):\n",
    "                    classes.append(5)\n",
    "                elif (temp == 'Sukun'):\n",
    "                    classes.append(6)\n",
    "                elif (temp == 'Shadda'):\n",
    "                    classes.append(7)\n",
    "        elif text[i - 1] not in ARABIC_DIACRITICS:\n",
    "            diacritics.append('')\n",
    "            classes.append(14)\n",
    "        \n",
    "    if text[-1] not in ARABIC_DIACRITICS:\n",
    "        diacritics.append('')\n",
    "        classes.append(14)\n",
    "    return diacritics, classes\n",
    "\n",
    "\n",
    "def extract_arabic_words2(text):\n",
    "    arabic_pattern = re.compile('[\\u0600-\\u06FF]+')\n",
    "    arabic_matches = arabic_pattern.findall(text)\n",
    "    result = ' '.join(arabic_matches)\n",
    "    processed_text = re.sub(r'[؛،\\.]+', '', result)\n",
    "    final_processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "    return final_processed_text\n",
    "\n",
    "\n",
    "input_file_path = \"train.txt\"  # Replace with your input file path\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    input_text = input_file.read()\n",
    "\n",
    "arabic_words = extract_arabic_words2(input_text)\n",
    "\n",
    "output_words = clear_diacritics(arabic_words)\n",
    "words = output_words.split()\n",
    "words2 = arabic_words.split()\n",
    "words_array = [list(word) for word in words]\n",
    "words_array2 = [list(word2) for word2 in words2]\n",
    "\n",
    "output_without_spaces = arabic_words.replace(\" \", \"\")\n",
    "output_without_spaces2 = output_words.replace(\" \", \"\")\n",
    "array_of_chars = [char for char in output_without_spaces]\n",
    "_,classes_extraction = extract_diacritics (output_without_spaces)\n",
    "\n",
    "\n",
    "num_feature = 100\n",
    "min_word_count = 1\n",
    "num_thread = 5\n",
    "window_size = 10\n",
    "down_sampling = 0.001\n",
    "iteration = 20\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model_fastText = FastText(words_array,\n",
    "                        vector_size=num_feature,\n",
    "                        window=window_size,\n",
    "                        min_count=min_word_count,\n",
    "                        workers=num_thread)\n",
    "\n",
    "\n",
    "j=0\n",
    "chars =[]\n",
    "char_vectors =[]\n",
    "char_classes=[]\n",
    "for word in words_array:\n",
    "  i=i+1\n",
    "  for char in word:\n",
    "    chars.append(char)\n",
    "    char_classes.append(classes_extraction[j])\n",
    "    vector = model_fastText.wv[char]\n",
    "    char_vectors.append(vector)\n",
    "    j=j+1\n",
    "\n",
    "print (j)\n",
    "print(chars[1])\n",
    "print(char_classes[1])\n",
    "print(char_vectors[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([3, 2])\n",
      "Final Hidden State Shape: torch.Size([1, 3, 32])\n",
      "Output Shape: torch.Size([3, 2])\n",
      "Final Hidden State Shape: torch.Size([1, 3, 32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-637fde8e8e6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;31m#train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;31m#evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-637fde8e8e6f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# loop over the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[1;31m# zero the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# class RNN_Model(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "#         super(RNN_Model, self).__init__()\n",
    "        \n",
    "#         # Defining some parameters\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         #Defining the layers\n",
    "#         # RNN Layer\n",
    "#         self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "#     def forward(self, x, h_prev):\n",
    "            \n",
    "#             batch_size = x.size(0)\n",
    "            \n",
    "#             #initialize hidden state with h_prev\n",
    "#             hidden = h_prev\n",
    "            \n",
    "#             #make sure hidden state dimensions are correct\n",
    "#             hidden = hidden.view(self.n_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "#             # Passing in the input and hidden state into the model and obtaining outputs\n",
    "#             out, hidden = self.rnn(x, hidden)\n",
    "#             # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "#             out = out.contiguous().view(-1, self.hidden_dim)\n",
    "#             out = self.fc(out)\n",
    "            \n",
    "#             return out, hidden\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        # Forward pass through the RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Select the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hn\n",
    "\n",
    "# Example usage\n",
    "input_size = 10  # Adjust based on your input features\n",
    "hidden_size = 32\n",
    "output_size = 2  # Adjust based on your task (e.g., classification)\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Example input (batch_size=3, sequence_length=5, input_size=10)\n",
    "sample_input = torch.randn(3, 5, 10)\n",
    "\n",
    "# Initial hidden state (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.zeros(1, 3, hidden_size)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "output, final_hidden = rnn_model(sample_input, h0)\n",
    "\n",
    "# Print the output shape and final hidden state shape\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Final Hidden State Shape:\", final_hidden.shape)\n",
    "#####################\n",
    "def train_model(model, train_loader):\n",
    "    \"\"\"\n",
    "    Function for training the model\n",
    "    \"\"\"\n",
    "    # define the optimization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    # define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # epochs\n",
    "    epochs = 10\n",
    "    # loop over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # initialize the hidden state\n",
    "        h0 = torch.zeros(1, 3, hidden_size)\n",
    "        # loop over the dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat, h0 = model(inputs, h0)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, labels)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "        # print the loss\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "#####################\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Function for evaluating the model\n",
    "    \"\"\"\n",
    "    # initialize the hidden state\n",
    "    h0 = torch.zeros(1, 3, hidden_size)\n",
    "    # initialize the accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # deactivating autograd\n",
    "    with torch.no_grad():\n",
    "        # loop over the test dataset\n",
    "        for inputs, labels in test_loader:\n",
    "            # compute the model output\n",
    "            yhat, h0 = model(inputs, h0)\n",
    "            # get predictions from the maximum value\n",
    "            _, predicted = torch.max(yhat.data, 1)\n",
    "            # update total\n",
    "            total += labels.size(0)\n",
    "            # update correct\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    # compute the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print the accuracy\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    \n",
    "# # evaluate the model\n",
    "# evaluate_model(rnn_model, test_loader)\n",
    "# #####################\n",
    "# def predict(model, test_loader):\n",
    "#     \"\"\"\n",
    "#     Function for predicting the results\n",
    "#     \"\"\"\n",
    "#     # initialize the hidden state\n",
    "#     h0 = torch.zeros(1, 3, hidden_size)\n",
    "#     # initialize the prediction list\n",
    "#     predictions = []\n",
    "#     # deactivating autograd\n",
    "#     with torch.no_grad():\n",
    "#         # loop over the test dataset\n",
    "#         for inputs, labels in test_loader:\n",
    "#             # compute the model output\n",
    "#             yhat, h0 = model(inputs, h0)\n",
    "#             # get predictions from the maximum value\n",
    "#             _, predicted = torch.max(yhat.data, 1)\n",
    "#             # append the batch prediction results\n",
    "#             predictions.append(predicted.numpy())\n",
    "#     # combine all the batch prediction results\n",
    "#     predictions = np.concatenate(predictions)\n",
    "#     # print the predictions\n",
    "#     print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_size = 10  # Adjust based on your input features\n",
    "hidden_size = 32\n",
    "output_size = 2  # Adjust based on your task (e.g., classification)\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Example input (batch_size=3, sequence_length=5, input_size=10)\n",
    "sample_input = torch.randn(3, 5, 10)\n",
    "\n",
    "# Initial hidden state (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.zeros(1, 3, hidden_size)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "output, final_hidden = rnn_model(sample_input, h0)\n",
    "\n",
    "# Print the output shape and final hidden state shape\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Final Hidden State Shape:\", final_hidden.shape)\n",
    "\n",
    "#create dummy train and test datasets with 3 samples, 5 timesteps, and 10 features\n",
    "train = torch.randn(3, 5, 10)\n",
    "test = torch.randn(3, 5, 10)\n",
    "\n",
    "#loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=3)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=3)\n",
    "\n",
    "#train the model\n",
    "train_model(rnn_model, train_loader)\n",
    "\n",
    "#evaluate the model\n",
    "evaluate_model(rnn_model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     \"\"\"Encoder layer to encode a sequence to a hidden state\"\"\"\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, flavor):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         input_size: int\n",
    "#         hidden_size: int\n",
    "#             Number of hidden units in the RNN model\n",
    "#         num_layers: int\n",
    "#             Number of layers in the RNN model\n",
    "#         flavor: str\n",
    "#             Takes 'rnn', 'lstm', or 'gru' values.\n",
    "#         \"\"\"\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.flavor = flavor\n",
    "\n",
    "#         if flavor == 'rnn':\n",
    "#             self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         elif flavor == 'lstm':\n",
    "#             self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         elif flavor == 'gru':\n",
    "#             self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid flavor. Choose 'rnn', 'lstm', or 'gru'.\")\n",
    "\n",
    "#     def forward(self, x, hidden):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the encoder layer.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x: torch.Tensor\n",
    "#             Input sequence tensor of shape (batch_size, sequence_length, input_size)\n",
    "#         hidden: torch.Tensor\n",
    "#             Initial hidden state tensor of shape (num_layers, batch_size, hidden_size)\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         output: torch.Tensor\n",
    "#             Output tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "#         hidden: torch.Tensor\n",
    "#             Updated hidden state tensor of shape (num_layers, batch_size, hidden_size)\n",
    "#         \"\"\"\n",
    "#         output, hidden = self.rnn(x, hidden)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         \"\"\"\n",
    "#         Initialize the hidden state tensor.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         batch_size: int\n",
    "#             Size of the batch\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         hidden: torch.Tensor\n",
    "#             Initial hidden state tensor of shape (num_layers, batch_size, hidden_size)\n",
    "#         \"\"\"\n",
    "#         if self.flavor == 'lstm':\n",
    "#             return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "#                     torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "#         else:\n",
    "#             return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, flavor):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.flavor = flavor\n",
    "\n",
    "        if flavor == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif flavor == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif flavor == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid flavor. Choose 'rnn', 'lstm', or 'gru'.\")\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.flavor == 'lstm':\n",
    "            return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                    torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tashkeel_Model(nn.Module):\n",
    "#     \"\"\" encoder layer + RNN layer\"\"\"\n",
    "#     def __init__(self, input_size, output_size, hidden_size, num_layers, flavor):\n",
    "#         super(Tashkeel_Model, self).__init__()\n",
    "#         self.encoder = Encoder(input_size, hidden_size, num_layers, flavor)\n",
    "#         self.rnn = RNN_Model(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"x shape is \", x.shape)\n",
    "#         #forward pass of the encoder\n",
    "#         h_minus_1 = self.encoder.init_hidden(x.shape[0])\n",
    "#         print(\"h_minus_1 shape is \", h_minus_1.shape)\n",
    "#         out, h_minus_1 = self.encoder(x, h_minus_1)\n",
    "#         print(\"encoder output shape is \", h_minus_1.shape)\n",
    "#         #adjust the shape of the output of the encoder\n",
    "#         h_minus_1 = h_minus_1.view(h_minus_1.shape[1], h_minus_1.shape[0], h_minus_1.shape[2])\n",
    "#         out, hidden = self.rnn(out, h_minus_1)\n",
    "#         return out, hidden\n",
    "\n",
    "class Tashkeel_Model(nn.Module):\n",
    "    def __init__(self, word_input_size, char_input_size, output_size, hidden_size, num_layers, flavor):\n",
    "        super(Tashkeel_Model, self).__init__()\n",
    "        self.word_encoder = Encoder(word_input_size, hidden_size, num_layers, flavor)\n",
    "        self.rnn = RNN_Model(char_input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, words, chars):\n",
    "        # Forward pass of the word encoder\n",
    "        h_minus_1 = self.word_encoder.init_hidden(words.shape[0])\n",
    "        out, h_minus_1 = self.word_encoder(words, h_minus_1)\n",
    "        # Adjust the shape of the output of the encoder\n",
    "        # Forward pass of the RNN\n",
    "        out, hidden = self.rnn(chars, h_minus_1)\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "def train_tashkeel_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move the data to the device that is used\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass the Encoder\n",
    "            output, _ = model(data)\n",
    "\n",
    "            # Calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss for progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TashkeelDataset(Dataset):\n",
    "    def __init__(self, word_data, char_data, labels):\n",
    "        self.word_data = word_data\n",
    "        self.char_data = char_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_sequence = self.word_data[idx]\n",
    "        char_sequence = self.char_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return torch.tensor(word_sequence), torch.tensor(char_sequence), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-60d140dea7f9>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(word_sequence), torch.tensor(char_sequence), torch.tensor(label)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-31df2f98f4a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtashkeel_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrain_tashkeel_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtashkeel_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-04b279a61ae5>\u001b[0m in \u001b[0;36mtrain_tashkeel_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;31m# Move the data to the device that is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word_size = 20\n",
    "char_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 300  # Adjust based on the number of classes for diacritics or the dimension of your embeddings\n",
    "num_layers = 1\n",
    "flavor = 'rnn'\n",
    "\n",
    "tashkeel_model = Tashkeel_Model(word_size, char_size, output_size, hidden_size, num_layers, flavor)\n",
    "\n",
    "dummy_word_data = torch.randn(32, 10, word_size)\n",
    "dummy_char_data = torch.randn(32, 10, char_size)\n",
    "dummy_labels = torch.randn(32, 10, output_size)\n",
    "\n",
    "dummy_dataset = TashkeelDataset(dummy_word_data, dummy_char_data, dummy_labels)\n",
    "dummy_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = torch.optim.Adam(tashkeel_model.parameters(), lr=0.001)\n",
    "\n",
    "train_tashkeel_model(tashkeel_model, dummy_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "# Define your DataLoader for training, including the input data\n",
    "\n",
    "# train_dataset = TashkeelDataset(word_data, char_data, labels)\n",
    "# #! TODO : add the target data\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "# optimizer = optim.Adam(tashkeel_model.parameters(), lr=0.001)\n",
    "# train_tashkeel_model(tashkeel_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "# dummy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_size = 300  # Adjust based on the dimension of your word embeddings\n",
    "hidden_dim = 128\n",
    "n_layers = 2  # Number of RNN layers\n",
    "\n",
    "word_encoder = WordEncoder(input_size, hidden_dim, n_layers)\n",
    "\n",
    "# Assuming 'input_sequence' is your input tensor\n",
    "output_sequence, hidden_state = word_encoder(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[ 0.042906   -0.02756045 -0.15988487 ...  0.06463267 -0.00134799\n",
      "  -0.07523397]\n",
      " [-0.17331032  0.11529611 -0.34257153 ...  0.02257426  0.08717281\n",
      "   0.01006125]\n",
      " [ 0.05427927 -0.08796314 -0.3034795  ...  0.06067655 -0.19971474\n",
      "   0.02589632]\n",
      " ...\n",
      " [-0.01609128  0.14161943 -0.5717865  ...  0.41028932 -0.2221118\n",
      "  -0.5745774 ]\n",
      " [-0.12516408  0.20515387 -0.6757796  ...  0.16081104  0.12018422\n",
      "  -0.57703876]\n",
      " [-0.12468552 -0.08162142 -0.47292602 ...  0.24188764 -0.19347623\n",
      "  -0.3513778 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "\n",
    "test_input_sentence = 'السلام عليكم ورحمة الله وبركاته'\n",
    "\n",
    "test_input_sentence_words = test_input_sentence.split(' ')\n",
    "print(test_input_sentence_words)\n",
    "\n",
    "# load the embedding model\n",
    "model = fasttext.load_model('word_embedding_model.bin')\n",
    "\n",
    "# get the embedding of the input sentence\n",
    "test_input_sentence_embedding = [\n",
    "    model[word] for word in test_input_sentence_words\n",
    "]\n",
    "print(test_input_sentence_embedding)\n",
    "\n",
    "# convert the embedding to tensor\n",
    "test_input_sentence_embedding = torch.tensor(test_input_sentence_embedding)\n",
    "print(test_input_sentence_embedding)\n",
    "\n",
    "# reshape the tensor\n",
    "test_input_sentence_embedding = test_input_sentence_embedding.view(1, -1, 100)\n",
    "print(test_input_sentence_embedding)\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class WordEncoder(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(WordEncoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         # Define the Encoder layer as bidirectional LSTM\n",
    "#         self.encoder = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "#         # Define the output layer for reconstruction\n",
    "#         self.reconstruction_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Forward pass through the Encoder\n",
    "        \n",
    "#         _, (hidden, _) = self.encoder(x)\n",
    "        \n",
    "#         # Concatenate the hidden states of both directions\n",
    "#         hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
    "\n",
    "#         # Pass the concatenated vector to the reconstruction layer\n",
    "#         reconstructed_x = self.reconstruction_layer(hidden)\n",
    "\n",
    "#         return reconstructed_x\n",
    "\n",
    "# def train_word_encoder(encoder_model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     encoder_model.to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         encoder_model.train()\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             # Move the data to the device that is used\n",
    "#             data = data.to(device)\n",
    "\n",
    "#             # Forward pass the Encoder\n",
    "#             output = encoder_model(data)\n",
    "            \n",
    "#             # Calculate the batch loss\n",
    "#             loss = criterion(output, target)\n",
    "            \n",
    "#             # Backpropagation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Print loss for progress\n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# input_size = 300  # Adjust based on the dimension of your word embeddings\n",
    "# hidden_size = 128\n",
    "# output_size = 300  # Adjust based on the number of classes for diacritics or the dimension of your embeddings\n",
    "\n",
    "# word_encoder_model = WordEncoder(input_size, hidden_size, output_size)\n",
    "\n",
    "# # Define your DataLoader for training, including the input data\n",
    "\n",
    "# criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "# optimizer = optim.Adam(word_encoder_model.parameters(), lr=0.001)\n",
    "\n",
    "# #load embeddings bin from fasttext\n",
    "# import fasttext\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class WordEmbeddingDataset(Dataset):\n",
    "#     def __init__(self, embeddings_bin_path):\n",
    "#         self.embeddings = fasttext.load_model(embeddings_bin_path)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.embeddings.words)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         word = self.embeddings.words[idx]\n",
    "#         embedding = torch.tensor(self.embeddings[word])\n",
    "\n",
    "#         return embedding, word\n",
    "    \n",
    "# train_dataset = WordEmbeddingDataset(\"word_embed_model.bin\")\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# train_word_encoder(word_encoder_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fasttext\n",
    "class WordEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(WordEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the Encoder layer as bidirectional LSTM\n",
    "        #TODO : FIX the bidirectional LSTM\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Define the output layer for reconstruction\n",
    "        self.reconstruction_layer = nn.Linear(2 * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the Encoder\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "\n",
    "        # # Concatenate the hidden states of both directions\n",
    "        # hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=0)  # Use dim=0 to concatenate along the batch dimension\n",
    "\n",
    "        # Pass the concatenated vector to the reconstruction layer\n",
    "        reconstructed_x = self.reconstruction_layer(hidden)\n",
    "\n",
    "        return reconstructed_x\n",
    "\n",
    "\n",
    "\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings_bin_path):\n",
    "        self.embeddings = fasttext.load_model(embeddings_bin_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.embeddings.words[idx]\n",
    "        embedding = torch.tensor(self.embeddings[word])\n",
    "\n",
    "        return embedding, embedding  # Using the embedding as target for reconstruction\n",
    "\n",
    "def train_word_encoder(encoder_model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder_model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        encoder_model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move the data to the device that is used\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass the Encoder\n",
    "            output = encoder_model(data)\n",
    "\n",
    "            # Calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss for progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Example usage\n",
    "input_size = 300  # Adjust based on the dimension of your word embeddings\n",
    "hidden_size = 128\n",
    "output_size = 300  # Adjust based on the number of classes for diacritics or the dimension of your embeddings\n",
    "\n",
    "word_encoder_model = WordEncoder(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define your DataLoader for training, including the input data\n",
    "train_dataset = WordEmbeddingDataset(\"word_embed_model.bin\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(word_encoder_model.parameters(), lr=0.001)\n",
    "\n",
    "train_word_encoder(word_encoder_model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
