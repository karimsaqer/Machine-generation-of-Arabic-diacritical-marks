{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Module\n",
    "\n",
    "This module focuses on extracting features for words and characters. It includes both word-level and character-level feature extraction techniques.\n",
    "\n",
    "## Word Level Features\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "The Bag of Words technique represents each word in a document as a vector, where each element of the vector corresponds to the frequency of a specific word in the document.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document. It takes into account both the frequency of the word in the document and the frequency of the word in the entire corpus.\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships between words. They can be learned from large text corpora or obtained from pre-trained models like BERT or ELMo, which provide contextualized word embeddings.\n",
    "\n",
    "## Character Level Features\n",
    "\n",
    "Character level features consider the characters within words, taking into account the characters preceding and following them.\n",
    "\n",
    "### Character Embeddings\n",
    "\n",
    "Character embeddings are dense vector representations of characters that capture the relationships between characters. They can be learned from the data or obtained from pre-trained models.\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding represents each character as a binary vector, where each element of the vector corresponds to a specific character. This technique is useful for capturing categorical information about characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %pip install seaborn\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(df, col):\n",
    "    # Create a bag of words\n",
    "    bow = CountVectorizer()\n",
    "    bow.fit(df[col])\n",
    "    bow = bow.transform(df[col])\n",
    "    bow_df = pd.DataFrame(bow.toarray())\n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df, col):\n",
    "    # Create a tfidf\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(df[col])\n",
    "    tfidf = tfidf.transform(df[col])\n",
    "    tfidf_df = pd.DataFrame(tfidf.toarray())\n",
    "    return tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.928964376449585, 'اللهِ'), (0.8942822813987732, 'عليه'), (0.8932053446769714, 'صلى'), (0.88192218542099, 'اللهُ'), (0.8680329322814941, 'وسلم'), (0.7963157296180725, 'الأَنْصَارِىِّ'), (0.7874833345413208, 'اللَّه'), (0.787224531173706, 'النّاسُ'), (0.7832015156745911, 'النُّعْمَانِ'), (0.7828898429870605, 'النُّعْمَانُ')]\n",
      "[(0.7418180704116821, 'النُّفُوسِ'), (0.7313854694366455, 'اللهُ'), (0.7193009853363037, 'السِّدْرِ'), (0.7180575728416443, 'التَّشَبُّهِ'), (0.7086499333381653, 'الله'), (0.7077236175537109, 'الْقُمَاشِ'), (0.7072348594665527, 'الشَّكْوَى'), (0.7053272724151611, 'الْكِبْرَ'), (0.7052369117736816, 'النُّفُوذِ'), (0.7025838494300842, 'الصِّبْيَانَ')]\n",
      "[(0.7427960634231567, 'النُّفُوسِ'), (0.7381163835525513, 'اللهُ'), (0.7364920377731323, 'السِّدْرِ'), (0.7363826632499695, 'التَّشَبُّهِ'), (0.7237018346786499, 'الطِّيبَ'), (0.7141523957252502, 'الدَّلْوِ'), (0.7117125988006592, 'الله'), (0.7079038619995117, 'الشَّكْوَى'), (0.7070366144180298, 'الْقُمَاشِ'), (0.7068793177604675, 'الطُّلُوعِ')]\n",
      "[(0.7415812015533447, 'النُّفُوسِ'), (0.7301138043403625, 'اللهُ'), (0.7191346883773804, 'السِّدْرِ'), (0.7170433402061462, 'التَّشَبُّهِ'), (0.7078521847724915, 'الْقُمَاشِ'), (0.7075499296188354, 'الله'), (0.7071104049682617, 'الشَّكْوَى'), (0.7053214907646179, 'الْكِبْرَ'), (0.704886794090271, 'النُّفُوذِ'), (0.7027411460876465, 'الصِّبْيَانَ')]\n",
      "[(0.7416667938232422, 'النُّفُوسِ'), (0.7307623624801636, 'اللهُ'), (0.7191076278686523, 'السِّدْرِ'), (0.7171295285224915, 'التَّشَبُّهِ'), (0.7086875438690186, 'الله'), (0.708391547203064, 'الْقُمَاشِ'), (0.7071662545204163, 'الشَّكْوَى'), (0.7058107852935791, 'الْكِبْرَ'), (0.7047825455665588, 'النُّفُوذِ'), (0.7030984163284302, 'الصِّبْيَانَ')]\n"
     ]
    }
   ],
   "source": [
    "# %pip install fasttext\n",
    "import fasttext\n",
    "def create_arabic_word_embedding(text_file, model_file):\n",
    "    # Train the FastText model on the Arabic text file\n",
    "    model = fasttext.train_unsupervised(text_file, model='skipgram')\n",
    "    #return feature vector for each word\n",
    "    model.save_model(model_file)\n",
    "    return model\n",
    "\n",
    "#test the model and print the most similar words\n",
    "def test_model(model_file):\n",
    "    model = fasttext.load_model(model_file)\n",
    "    print(model.get_nearest_neighbors('الله'))\n",
    "    print(model.get_nearest_neighbors('الملك'))\n",
    "    print(model.get_nearest_neighbors('الملكة'))\n",
    "    print(model.get_nearest_neighbors('الملكي'))\n",
    "    print(model.get_nearest_neighbors('الملكية'))\n",
    "\n",
    "text_file = 'processed_output.txt'\n",
    "model_file = 'model.bin'\n",
    "create_arabic_word_embedding(text_file, model_file)\n",
    "test_model(model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8729323148727417, 'كالصلاة'), (0.8679752945899963, 'لصلاة'), (0.837247908115387, 'والصلاة'), (0.8103158473968506, 'وصلاة'), (0.807507336139679, 'بصلاة'), (0.8005672693252563, 'صلاة'), (0.7991220355033875, 'للصلاة'), (0.7942743897438049, 'كصلاة'), (0.7561188340187073, 'فصلاته'), (0.7496492862701416, 'بالصلاة')]\n",
      "[(0.8781991600990295, 'صيام'), (0.8510888814926147, 'كصيام'), (0.8345058560371399, 'فصيام'), (0.8330397009849548, 'بصيام'), (0.8206717371940613, 'بالصيام'), (0.7958154678344727, 'وصيام'), (0.7822644114494324, 'صيامه'), (0.7596244812011719, 'وأيام'), (0.7555518746376038, 'الصوم'), (0.7435011267662048, 'الإطعام')]\n",
      "[(0.9087247848510742, 'كالزكاة'), (0.8874373435974121, 'للزكاة'), (0.8786138892173767, 'والزكاة'), (0.8772928714752197, 'زكاة'), (0.8646826148033142, 'كزكاة'), (0.8360320925712585, 'وزكاة'), (0.7191078662872314, 'زكاته'), (0.6740335822105408, 'زكاتها'), (0.6733525395393372, 'الفطرة'), (0.6722766160964966, 'الغنى')]\n",
      "[(0.8515108823776245, 'العمرة'), (0.8395167589187622, 'كالحج'), (0.8234612941741943, 'بالعمرة'), (0.8176628351211548, 'لحج'), (0.8165479898452759, 'للعمرة'), (0.8149896264076233, 'لعمرة'), (0.808195948600769, 'والعمرة'), (0.7965331077575684, 'الحجج'), (0.7919480204582214, 'للحج'), (0.7873984575271606, 'بعمرة')]\n",
      "[(0.9169186353683472, 'لإسلام'), (0.8894520401954651, 'والإسلام'), (0.8668771386146545, 'للإسلام'), (0.8655571341514587, 'بالإسلام'), (0.8638903498649597, 'إسلام'), (0.8364660143852234, 'وإسلام'), (0.8315708041191101, 'بإسلام'), (0.7592645287513733, 'وإسلامه'), (0.7589341998100281, 'إسلامه'), (0.7555813193321228, 'إسلامهن')]\n",
      "[(0.8807896375656128, 'إيمان'), (0.8693927526473999, 'بالإيمان'), (0.8476983308792114, 'الإيماء'), (0.8326382040977478, 'إيمانه'), (0.7834004163742065, 'الإحسان'), (0.7698593735694885, 'إيمانا'), (0.7579582333564758, 'الإظهار'), (0.7574423551559448, 'الإله'), (0.7505175471305847, 'الاستخفاف'), (0.7474504113197327, 'يمان')]\n",
      "[(0.812983512878418, 'والإحسان'), (0.8045915365219116, 'الإله'), (0.7834006547927856, 'الإيمان'), (0.7826696038246155, 'إحسان'), (0.7684891223907471, 'الإلهام'), (0.7604241371154785, 'الإحجاج'), (0.7584832310676575, 'بإحسان'), (0.7525087594985962, 'الإذخر'), (0.7382882833480835, 'الإحاطة'), (0.7201023101806641, 'الاستخفاف')]\n",
      "[(0.8396891951560974, 'بالسلام'), (0.7888349294662476, 'والسلام'), (0.7822648882865906, 'سلام'), (0.7672742605209351, 'وسلام'), (0.7454043626785278, 'بسلام'), (0.7193230986595154, 'السلاح'), (0.6883274912834167, 'السلامة'), (0.6518991589546204, 'علام'), (0.6370881795883179, 'سلامه'), (0.6200007200241089, 'السلس')]\n",
      "[(0.8911059498786926, 'كالمسلم'), (0.8814967274665833, 'المسلمات'), (0.8583637475967407, 'لمسلم'), (0.8381639719009399, 'المسلمة'), (0.8360686302185059, 'والمسلم'), (0.8209950923919678, 'بالمسلم'), (0.8131820559501648, 'للمسلم'), (0.8037762641906738, 'كمسلم'), (0.7971572279930115, 'فمسلم'), (0.7958024144172668, 'المسلف')]\n",
      "[(0.8322980403900146, 'يجوزه'), (0.7493318915367126, 'يتجوز'), (0.7452088594436646, 'جوز'), (0.73484206199646, 'فجوز'), (0.7134251594543457, 'فيجوز'), (0.7000840306282043, 'أيجوز'), (0.68218594789505, 'وتجوز'), (0.6747153997421265, 'ويجوز'), (0.6670426726341248, 'تجوز'), (0.6656419634819031, 'فتجوز')]\n",
      "[(0.7690654993057251, 'وجب'), (0.7268897891044617, 'ويجب'), (0.7112703323364258, 'نوجب'), (0.6661739349365234, 'فيجب'), (0.6617647409439087, 'تجب'), (0.6583831310272217, 'فواجب'), (0.6549530029296875, 'لوجب'), (0.6538200378417969, 'جب'), (0.6492347717285156, 'ووجب'), (0.6423161029815674, 'فيوجب')]\n",
      "[(0.8511277437210083, 'فيحرم'), (0.8443931341171265, 'يحرمن'), (0.8080747723579407, 'ويحرم'), (0.7825798392295837, 'كمحرم'), (0.7769650220870972, 'يحرمه'), (0.7610647082328796, 'حرم'), (0.7594454884529114, 'لمحرم'), (0.7439813613891602, 'وحرم'), (0.7325904369354248, 'وتحرم'), (0.7180548310279846, 'ومحرم')]\n"
     ]
    }
   ],
   "source": [
    "#more tests\n",
    "text_file = 'words.txt'\n",
    "model_file = 'model.bin'\n",
    "create_arabic_word_embedding(text_file, model_file)\n",
    "model = fasttext.load_model(model_file)\n",
    "print(model.get_nearest_neighbors('الصلاة'))\n",
    "print(model.get_nearest_neighbors('الصيام'))\n",
    "print(model.get_nearest_neighbors('الزكاة'))\n",
    "print(model.get_nearest_neighbors('الحج'))\n",
    "print(model.get_nearest_neighbors('الإسلام'))\n",
    "print(model.get_nearest_neighbors('الإيمان'))\n",
    "print(model.get_nearest_neighbors('الإحسان'))\n",
    "print(model.get_nearest_neighbors('السلام'))\n",
    "print(model.get_nearest_neighbors('المسلم'))\n",
    "print(model.get_nearest_neighbors('يجوز'))\n",
    "print(model.get_nearest_neighbors('يجب'))\n",
    "print(model.get_nearest_neighbors('يحرم'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a function to extract contexual embeddings of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-6d93bff402ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Example usage:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0minput_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"processed_output.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_contextual_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# 'embeddings' is a list where each element corresponds to the contextual embeddings for words in a sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-6d93bff402ae>\u001b[0m in \u001b[0;36mextract_contextual_embeddings\u001b[1;34m(input_file_path, model_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Load pre-trained BERT tokenizer and model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Read input file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"_from_config\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lap2\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
